# PruningEncoder Loading Guide

PruningEncoderã¯è¤‡æ•°ã®æ–¹æ³•ã§ãƒ­ãƒ¼ãƒ‰ã§ãã€ç”¨é€”ã«å¿œã˜ã¦æœ€é©ãªæ–¹æ³•ã‚’é¸æŠã§ãã¾ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨ï¼ˆç‰¹åˆ¥ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸è¦ï¼‰

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# ranking_modelã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
model = AutoModelForSequenceClassification.from_pretrained("path/to/saved_model/ranking_model")
tokenizer = AutoTokenizer.from_pretrained("path/to/saved_model/ranking_model")

# é€šå¸¸ã®æ¨è«–
inputs = tokenizer("ã‚¯ã‚¨ãƒª", "æ–‡æ›¸", return_tensors="pt")
outputs = model(**inputs)
score = torch.sigmoid(outputs.logits).item()
```

## ğŸ“Š å…¨ã¦ã®èª­ã¿è¾¼ã¿æ–¹æ³•

### 1. ãƒ•ãƒ«PruningEncoderï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰

```python
from sentence_transformers.pruning import PruningEncoder

model = PruningEncoder.from_pretrained("path/to/saved_model")

# ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ä»˜ãæ¨è«–
outputs = model.predict_with_pruning([("ã‚¯ã‚¨ãƒª", "æ–‡æ›¸")])
print(f"ã‚¹ã‚³ã‚¢: {outputs[0].ranking_scores}")
print(f"åœ§ç¸®ç‡: {outputs[0].compression_ratio}")
```

### 2. ãƒ™ãƒ¼ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨™æº–Transformersï¼‰

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# æ–¹æ³•A: ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›´æ¥æŒ‡å®š
model = AutoModelForSequenceClassification.from_pretrained("path/to/saved_model/ranking_model")

# æ–¹æ³•B: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨
pruning_model = PruningEncoder.from_pretrained("path/to/saved_model")
pruning_model.export_ranking_model("./exported_model")
model = AutoModelForSequenceClassification.from_pretrained("./exported_model")
```

### 3. AutoModelçµ±åˆï¼ˆè‡ªå‹•ç™»éŒ²ï¼‰

```python
import sentence_transformers  # è‡ªå‹•ç™»éŒ²

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("path/to/saved_model")
# trust_remote_code=True ä¸è¦ï¼
```

### 4. CrossEncoderäº’æ›

```python
import sentence_transformers
from sentence_transformers import CrossEncoder

model = CrossEncoder("path/to/saved_model")
scores = model.predict([("ã‚¯ã‚¨ãƒª", "æ–‡æ›¸")])
```

## ğŸ¯ ä½¿ã„åˆ†ã‘ã‚¬ã‚¤ãƒ‰

| ç”¨é€” | æ¨å¥¨æ–¹æ³• | ã‚¤ãƒ³ãƒãƒ¼ãƒˆ | ãƒ¡ãƒªãƒƒãƒˆ |
|------|----------|------------|----------|
| ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ã¿ | `/ranking_model` | transformersã®ã¿ | æœ€å°é™ã€é«˜é€Ÿ |
| ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¿…è¦ | PruningEncoder | sentence_transformers | ãƒ•ãƒ«æ©Ÿèƒ½ |
| æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ | CrossEncoder | sentence_transformers | APIäº’æ›æ€§ |
| æŸ”è»Ÿãªä½¿ç”¨ | AutoModel + ç™»éŒ² | sentence_transformers | æ¨™æº–ãƒ‘ã‚¿ãƒ¼ãƒ³ |

## ğŸ’¾ ä¿å­˜ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
saved_model/
â”œâ”€â”€ config.json                    # PruningEncoderè¨­å®š
â”œâ”€â”€ pruning_encoder_config.json    # è©³ç´°è¨­å®š
â”œâ”€â”€ modeling_pruning_encoder.py    # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ¼ãƒ‰ï¼ˆauto_mapç”¨ï¼‰
â”œâ”€â”€ tokenizer files               # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
â”œâ”€â”€ README.md                     # ä½¿ç”¨ã‚¬ã‚¤ãƒ‰ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
â”œâ”€â”€ ranking_model/                # â­ ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå®Œå…¨ãªTransformersãƒ¢ãƒ‡ãƒ«ï¼‰
â”‚   â”œâ”€â”€ config.json              # ModernBertConfigç­‰
â”‚   â”œâ”€â”€ model.safetensors        # ãƒ¢ãƒ‡ãƒ«é‡ã¿
â”‚   â””â”€â”€ tokenizer files          # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
â””â”€â”€ pruning_head/                 # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ˜ãƒƒãƒ‰
    â””â”€â”€ pytorch_model.bin
```

## ğŸ”‘ é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«åˆ©ç”¨å¯èƒ½**: `/ranking_model`ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å®Œå…¨ãªTransformersãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜
2. **ç‰¹åˆ¥ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸è¦**: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ä½¿ç”¨ã™ã‚‹å ´åˆ
3. **åŒã˜é‡ã¿ã€ç•°ãªã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**: ç”¨é€”ã«å¿œã˜ã¦é¸æŠå¯èƒ½
4. **å¾Œæ–¹äº’æ›æ€§ç¶­æŒ**: æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¯å…¨ã¦å‹•ä½œ

## ğŸ“ å®Ÿè£…ä¾‹

### ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¿ã‚¹ã‚¯
```python
# æœ€å°é™ã®ã‚³ãƒ¼ãƒ‰ - sentence_transformersã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸è¦ï¼
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("model_path/ranking_model")
tokenizer = AutoTokenizer.from_pretrained("model_path/ranking_model")
```

### RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
```python
from sentence_transformers.pruning import PruningEncoder

model = PruningEncoder.from_pretrained("model_path")
outputs = model.predict_with_pruning(query_doc_pairs, pruning_threshold=0.5)

for output in outputs:
    print(f"ã‚¹ã‚³ã‚¢: {output.ranking_scores}")
    print(f"åœ§ç¸®ç‡: {output.compression_ratio}%")
    print(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ: {output.pruned_documents[0]}")
```