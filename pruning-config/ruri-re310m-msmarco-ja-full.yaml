model_args:
  model_name_or_path: "cl-nagoya/ruri-v3-reranker-310m"
  classifier_dropout: 0.0

data_args:
  dataset_name: "hotchpotch/wip-msmarco-context-relevance"
  subset: "msmarco-ja-full"
  teacher_model_name: "japanese-reranker-xsmall-v2"

training_args:
  overwrite_output_dir: true
  optimizer: "adafactor"

  # Training parameters
  learning_rate: 5.0e-5
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 16
  max_grad_norm: 1.0

  # Optimizer and scheduler
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1

  # Logging and saving
  logging_steps: 100
  save_steps: 500
  save_total_limit: 3

  # Mixed precision
  fp16: false
  bf16: true

  # Other settings
  dataloader_num_workers: 8
  load_best_model_at_end: true
  num_train_epochs: 1

  # eval
  per_device_eval_batch_size: 16
  eval_steps: 500

  # Reporting
  report_to: ["wandb"]

