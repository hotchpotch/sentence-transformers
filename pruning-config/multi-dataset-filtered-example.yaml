model_args:
  model_name_or_path: "hotchpotch/japanese-reranker-xsmall-v2"
  mode: "reranking_pruning"
  classifier_dropout: 0.1

data_args:
  # Apply filtering to all datasets - limit to 4 items per row and remove all-zero relevance rows
  filter_zero_relevance_max_items: 4
  
  # Multiple datasets configuration
  datasets:
    - dataset_name: "hotchpotch/wip-msmarco-context-relevance"
      subset: "msmarco-ja-minimal"
      teacher_column: "teacher_scores.japanese-reranker-xsmall-v2"
    
    - dataset_name: "hotchpotch/wip-japanese-context-relevance"
      subset: "msmarco-ja-minimal"
      teacher_column: "teacher_scores.japanese-reranker-xsmall-v2"

training_args:
  output_dir: "./output/multi-dataset-filtered-model"
  overwrite_output_dir: true
  
  # Training parameters
  learning_rate: 5.0e-5
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  
  # Pruning loss weights
  ranking_weight: 0.05
  pruning_weight: 1.0
  
  # Optimizer
  optim: "adafactor"
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  # Logging and evaluation
  logging_steps: 100
  eval_steps: 500
  save_steps: 500
  save_total_limit: 5
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  
  # Mixed precision
  fp16: false
  bf16: true
  
  # Other settings
  dataloader_num_workers: 8
  report_to: ["wandb"]