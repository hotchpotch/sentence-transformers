2025-07-09 10:24:06,993 - __main__ - INFO - Starting reranking + pruning training with minimal dataset
2025-07-09 10:24:06,993 - __main__ - INFO - Loading dataset: hotchpotch/wip-query-context-pruner-with-teacher-scores/ja-minimal
2025-07-09 10:24:11,708 - __main__ - INFO - Train dataset size: 9881
2025-07-09 10:24:11,708 - __main__ - INFO - Eval dataset size: 59
2025-07-09 10:24:11,708 - __main__ - INFO - Dataset columns: ['id', 'query', 'texts', 'chunks_pos', 'labels', 'dataset_name', 'relevant_chunks', 'teacher_scores_japanese-reranker-xsmall-v2']
2025-07-09 10:24:11,708 - __main__ - INFO - Initializing PruningEncoder in reranking_pruning mode with base model: hotchpotch/japanese-reranker-xsmall-v2
2025-07-09 10:24:13,947 - __main__ - INFO - Training arguments: {'output_dir': './output/reranking_pruning_minimal_20250709_102413', 'num_epochs': 5, 'batch_size': 16, 'learning_rate': 2e-05, 'warmup_ratio': 0.1, 'weight_decay': 0.01, 'gradient_accumulation_steps': 1, 'max_grad_norm': 1.0, 'logging_steps': 10, 'eval_steps': 50, 'save_steps': 50, 'save_total_limit': 3, 'seed': 42, 'fp16': True, 'dataloader_num_workers': 2}
2025-07-09 10:24:13,948 - __main__ - INFO - Starting training...
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO - ***** Running training *****
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO -   Num examples = 9881
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO -   Num epochs = 5
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO -   Batch size = 16
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO -   Gradient accumulation steps = 1
2025-07-09 10:24:13,948 - sentence_transformers.pruning.trainer - INFO -   Total optimization steps = 3090
