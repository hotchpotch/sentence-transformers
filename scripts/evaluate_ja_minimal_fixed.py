#!/usr/bin/env python3
"""
ja-minimal ã§å­¦ç¿’ã—ãŸ Provence ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ­£ã—ãè©•ä¾¡
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from datasets import load_dataset
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# ä¿®æ­£ç‰ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from sentence_transformers.provence.encoder_token_pruning import ProvenceEncoder

def evaluate_model(model_path: str, dataset_subset: str = 'ja-minimal'):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    
    print(f"=== {dataset_subset} ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ï¼ˆä¿®æ­£ç‰ˆï¼‰ ===")
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰ä¸­...")
    dataset = load_dataset('hotchpotch/wip-query-context-pruner-with-teacher-scores', dataset_subset)
    test_data = dataset['test']
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ä»¶")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        model = ProvenceEncoder.from_pretrained(model_path)
        model.eval()
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    print("ğŸ” è©•ä¾¡å®Ÿè¡Œä¸­...")
    
    all_predictions = []
    all_labels = []
    all_ranking_scores = []
    all_teacher_scores = []
    compression_results = []
    
    with torch.no_grad():
        for i, item in enumerate(test_data):
            if i % 10 == 0:
                print(f"  é€²æ—: {i}/{len(test_data)}")
            
            query = item['query']
            texts = item['texts']
            labels = item['labels']
            teacher_scores = item['teacher_scores_japanese-reranker-xsmall-v2']
            
            # å„ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã‚’è©•ä¾¡
            for j, (text, label, teacher_score) in enumerate(zip(texts, labels, teacher_scores)):
                try:
                    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢äºˆæ¸¬
                    ranking_score = model.predict([(query, text)])[0]
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ï¼ˆè¤‡æ•°é–¾å€¤ã§è©•ä¾¡ï¼‰
                    for threshold in [0.1, 0.3, 0.5, 0.7]:
                        result = model.predict_with_token_pruning(
                            (query, text),
                            pruning_threshold=threshold,
                            return_documents=True
                        )
                        
                        compression_results.append({
                            'threshold': threshold,
                            'compression_ratio': result['compression_ratio'],
                            'ranking_score': result['ranking_score'],
                            'label': label,
                            'teacher_score': teacher_score,
                            'is_positive': label == 1,
                            'num_kept_tokens': result['num_kept_tokens'],
                            'num_total_tokens': result['num_total_tokens']
                        })
                    
                    all_predictions.append(ranking_score)
                    all_labels.append(label)
                    all_ranking_scores.append(ranking_score)
                    all_teacher_scores.append(teacher_score)
                    
                except Exception as e:
                    print(f"    âš ï¸  ã‚¨ãƒ©ãƒ¼ (item {i}, text {j}): {e}")
                    continue
    
    # çµæœåˆ†æ
    print("\nğŸ“Š è©•ä¾¡çµæœ:")
    
    # 1. ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # ãƒã‚¤ãƒŠãƒªåˆ†é¡ã¨ã—ã¦è©•ä¾¡ï¼ˆé–¾å€¤0.5ï¼‰
    binary_preds = (all_predictions > 0.5).astype(int)
    accuracy = accuracy_score(all_labels, binary_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, binary_preds, average='binary', zero_division=0
    )
    
    print(f"\nğŸ¯ ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½:")
    print(f"  Accuracy: {accuracy:.3f}")
    print(f"  Precision: {precision:.3f}")
    print(f"  Recall: {recall:.3f}")
    print(f"  F1: {f1:.3f}")
    
    # 2. ã‚¹ã‚³ã‚¢ç›¸é–¢
    pos_mask = all_labels == 1
    neg_mask = all_labels == 0
    
    if np.sum(pos_mask) > 0:
        pos_score_mean = np.mean(all_predictions[pos_mask])
        pos_teacher_mean = np.mean(np.array(all_teacher_scores)[pos_mask])
        print(f"  POSäºˆæ¸¬ã‚¹ã‚³ã‚¢å¹³å‡: {pos_score_mean:.3f} (æ•™å¸«: {pos_teacher_mean:.3f})")
    
    if np.sum(neg_mask) > 0:
        neg_score_mean = np.mean(all_predictions[neg_mask])
        neg_teacher_mean = np.mean(np.array(all_teacher_scores)[neg_mask])
        print(f"  NEGäºˆæ¸¬ã‚¹ã‚³ã‚¢å¹³å‡: {neg_score_mean:.3f} (æ•™å¸«: {neg_teacher_mean:.3f})")
    
    # 3. åœ§ç¸®æ€§èƒ½ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ï¼‰
    print(f"\nâœ‚ï¸  ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½:")
    
    for threshold in [0.1, 0.3, 0.5, 0.7]:
        threshold_results = [r for r in compression_results if r['threshold'] == threshold]
        
        if threshold_results:
            # POS/NEGåˆ¥ã®åœ§ç¸®ç‡
            pos_results = [r for r in threshold_results if r['is_positive']]
            neg_results = [r for r in threshold_results if not r['is_positive']]
            
            all_compression = [r['compression_ratio'] for r in threshold_results]
            pos_compression = [r['compression_ratio'] for r in pos_results] if pos_results else []
            neg_compression = [r['compression_ratio'] for r in neg_results] if neg_results else []
            
            # ãƒˆãƒ¼ã‚¯ãƒ³çµ±è¨ˆ
            all_kept = [r['num_kept_tokens'] for r in threshold_results]
            all_total = [r['num_total_tokens'] for r in threshold_results]
            
            print(f"\n  é–¾å€¤ {threshold}:")
            print(f"    å…¨ä½“åœ§ç¸®ç‡: {np.mean(all_compression):.1%} Â± {np.std(all_compression):.1%}")
            print(f"    å¹³å‡ä¿æŒãƒˆãƒ¼ã‚¯ãƒ³æ•°: {np.mean(all_kept):.1f}/{np.mean(all_total):.1f}")
            
            if pos_compression:
                pos_kept = [r['num_kept_tokens'] for r in pos_results]
                pos_total = [r['num_total_tokens'] for r in pos_results]
                print(f"    POSåœ§ç¸®ç‡: {np.mean(pos_compression):.1%} Â± {np.std(pos_compression):.1%}")
                print(f"    POSä¿æŒãƒˆãƒ¼ã‚¯ãƒ³æ•°: {np.mean(pos_kept):.1f}/{np.mean(pos_total):.1f}")
            
            if neg_compression:
                neg_kept = [r['num_kept_tokens'] for r in neg_results]
                neg_total = [r['num_total_tokens'] for r in neg_results]
                print(f"    NEGåœ§ç¸®ç‡: {np.mean(neg_compression):.1%} Â± {np.std(neg_compression):.1%}")
                print(f"    NEGä¿æŒãƒˆãƒ¼ã‚¯ãƒ³æ•°: {np.mean(neg_kept):.1f}/{np.mean(neg_total):.1f}")
    
    # 4. ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›
    print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ› (é–¾å€¤0.3):")
    
    sample_count = 0
    for item in test_data[:3]:  # æœ€åˆã®3ã¤ã®ã‚µãƒ³ãƒ—ãƒ«
        query = item['query']
        pos_text = item['texts'][0]  # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã¯å¿…ãšPOS
        
        try:
            result = model.predict_with_token_pruning(
                (query, pos_text),
                pruning_threshold=0.3,
                return_documents=True
            )
            
            print(f"\n  ã‚µãƒ³ãƒ—ãƒ« {sample_count + 1}:")
            print(f"    Query: {query}")
            print(f"    å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {len(pos_text)} æ–‡å­—")
            print(f"    åœ§ç¸®å¾Œé•·: {len(result['pruned_document'])} æ–‡å­—")
            print(f"    åœ§ç¸®ç‡: {result['compression_ratio']:.1%}")
            print(f"    ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢: {result['ranking_score']:.3f}")
            print(f"    ä¿æŒãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['num_kept_tokens']}/{result['num_total_tokens']}")
            print(f"    åœ§ç¸®å¾Œ: {result['pruned_document'][:200]}{'...' if len(result['pruned_document']) > 200 else ''}")
            
            sample_count += 1
            
        except Exception as e:
            print(f"    âš ï¸  ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\nâœ… è©•ä¾¡å®Œäº†")


def main():
    # å­¦ç¿’å®Œäº†ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    output_dir = "./outputs/provence-ja-minimal"
    
    # best modelã¾ãŸã¯æœ€æ–°ã®checkpointã‚’ä½¿ç”¨
    best_model_path = None
    final_model_path = os.path.join(output_dir, "final-model")
    
    # best checkpointã‚’æ¢ã™ï¼ˆç•ªå·ãŒå¤§ãã„æ–¹ã‚’å„ªå…ˆï¼‰
    if os.path.exists(output_dir):
        best_checkpoints = sorted(Path(output_dir).glob("checkpoint-*-best"), 
                                 key=lambda x: int(x.name.split('-')[1]))
        if best_checkpoints:
            best_model_path = str(best_checkpoints[-1])
    
    # è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
    if best_model_path and os.path.exists(best_model_path):
        model_path = best_model_path
        print(f"ğŸ† Best ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_path}")
    elif os.path.exists(final_model_path):
        model_path = final_model_path
        print(f"ğŸ”š Final ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_path}")
    else:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {output_dir}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluate_model(model_path, 'ja-minimal')


if __name__ == "__main__":
    main()