#!/usr/bin/env python3
"""
ja-minimal ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹é€ ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

from datasets import load_dataset
import json

def main():
    print("=== ja-minimal ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ç¢ºèª ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
    try:
        dataset = load_dataset('hotchpotch/wip-query-context-pruner-with-teacher-scores', 'ja-minimal')
        print("âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # åŸºæœ¬æƒ…å ±
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸºæœ¬æƒ…å ±:")
    for split_name, split_data in dataset.items():
        print(f"  {split_name}: {len(split_data):,} ä»¶")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    print(f"\nğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    sample = dataset['train'][0]
    
    print(f"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸€è¦§: {list(sample.keys())}")
    print(f"\nID: {sample['id']}")
    print(f"Query: {sample['query']}")
    print(f"Dataset name: {sample['dataset_name']}")
    print(f"Textsæ•°: {len(sample['texts'])}")
    print(f"Labels: {sample['labels']}")
    print(f"Chunks_posæ•°: {len(sample['chunks_pos'])}")
    print(f"Relevant_chunksæ•°: {len(sample['relevant_chunks'])}")
    print(f"Teacher_scoresæ•°: {len(sample['teacher_scores_japanese-reranker-xsmall-v2'])}")
    
    # è©³ç´°ç¢ºèª
    print(f"\nğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿æ§‹é€ :")
    for i, (text, chunks_pos, relevant_chunks, teacher_score, label) in enumerate(
        zip(sample['texts'], sample['chunks_pos'], sample['relevant_chunks'], 
            sample['teacher_scores_japanese-reranker-xsmall-v2'], sample['labels'])
    ):
        print(f"\n  Text {i} (label={label}, score={teacher_score:.3f}):")
        print(f"    Texté•·: {len(text)}")
        print(f"    Chunksæ•°: {len(chunks_pos)}")
        print(f"    Relevant chunks: {relevant_chunks}")
        print(f"    Text preview: {text[:100]}...")
        
        # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã‚‚ç¢ºèª
        if len(chunks_pos) > 0:
            print(f"    ãƒãƒ£ãƒ³ã‚¯ä¾‹:")
            for j, (start, end) in enumerate(chunks_pos[:3]):  # æœ€åˆã®3ãƒãƒ£ãƒ³ã‚¯ã®ã¿
                chunk_text = text[start:end].strip()
                is_relevant = j in relevant_chunks
                print(f"      [{j}] ({start}-{end}) {'âœ…' if is_relevant else 'âŒ'}: {chunk_text[:50]}...")

    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
    train_data = dataset['train']
    
    # ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ
    pos_count = sum(1 for item in train_data if item['labels'][0] == 1)  # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆã¯å¿…ãšPOS
    total_texts = len(train_data) * 5  # 1ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã5ãƒ†ã‚­ã‚¹ãƒˆ
    total_pos = sum(sum(labels) for labels in train_data['labels'])
    print(f"  POSç‡: {total_pos}/{total_texts} = {total_pos/total_texts:.1%}")
    
    # æ•™å¸«ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
    all_pos_scores = []
    all_neg_scores = []
    
    for item in train_data:
        for label, score in zip(item['labels'], item['teacher_scores_japanese-reranker-xsmall-v2']):
            if label == 1:
                all_pos_scores.append(score)
            else:
                all_neg_scores.append(score)
    
    if all_pos_scores:
        print(f"  POSæ•™å¸«ã‚¹ã‚³ã‚¢: å¹³å‡={sum(all_pos_scores)/len(all_pos_scores):.3f}, ä»¶æ•°={len(all_pos_scores)}")
    if all_neg_scores:
        print(f"  NEGæ•™å¸«ã‚¹ã‚³ã‚¢: å¹³å‡={sum(all_neg_scores)/len(all_neg_scores):.3f}, ä»¶æ•°={len(all_neg_scores)}")
    
    # relevant_chunksçµ±è¨ˆ
    total_relevant_chunks = 0
    total_chunks = 0
    
    for item in train_data:
        for chunks_pos, relevant_chunks in zip(item['chunks_pos'], item['relevant_chunks']):
            total_chunks += len(chunks_pos)
            total_relevant_chunks += len(relevant_chunks)
    
    print(f"  é–¢é€£ãƒãƒ£ãƒ³ã‚¯ç‡: {total_relevant_chunks}/{total_chunks} = {total_relevant_chunks/total_chunks:.1%}")
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ ç¢ºèªå®Œäº†")


if __name__ == "__main__":
    main()