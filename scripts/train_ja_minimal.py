#!/usr/bin/env python3
"""
ja-minimal ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã® Provence ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import logging
from pathlib import Path
from datasets import load_dataset

from sentence_transformers.provence import (
    ProvenceEncoder,
    ProvenceTrainer,
    ProvenceChunkBasedDataCollator
)
from sentence_transformers.provence.losses_chunk_based import ProvenceChunkBasedLoss

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def convert_dataset_format(hf_dataset):
    """
    HuggingFace ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Provence å­¦ç¿’å½¢å¼ã«å¤‰æ›
    """
    converted_data = []
    
    for item in hf_dataset:
        # teacher_scores ã®ã‚­ãƒ¼åã‚’å¤‰æ›´
        teacher_scores = item['teacher_scores_japanese-reranker-xsmall-v2']
        
        converted_item = {
            'query': item['query'],
            'texts': item['texts'],
            'chunks_pos': item['chunks_pos'],
            'relevant_chunks': item['relevant_chunks'],
            'ranking_labels': item['labels'],  # labels -> ranking_labels
            'teacher_scores': teacher_scores,
            'dataset_name': item['dataset_name'],
            'id': item['id']
        }
        converted_data.append(converted_item)
    
    return converted_data

def main():
    print("=== ja-minimal ã§ã® Provence ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = "./outputs/provence-ja-minimal"
    os.makedirs(output_dir, exist_ok=True)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰ä¸­...")
    dataset = load_dataset('hotchpotch/wip-query-context-pruner-with-teacher-scores', 'ja-minimal')
    
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å½¢å¼å¤‰æ›ä¸­...")
    train_data = convert_dataset_format(dataset['train'])
    eval_data = convert_dataset_format(dataset['validation'])
    
    print(f"âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(train_data):,} ä»¶")
    print(f"âœ… è©•ä¾¡ãƒ‡ãƒ¼ã‚¿: {len(eval_data):,} ä»¶")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = ProvenceEncoder(
        model_name_or_path="hotchpotch/japanese-reranker-xsmall-v2",
        num_labels=1,
        max_length=512,
        pruning_config={
            "dropout": 0.1,
            "sentence_pooling": "mean"
        }
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
    data_collator = ProvenceChunkBasedDataCollator(
        tokenizer=model.tokenizer,
        max_length=512,
        padding=True,
        truncation=True
    )
    
    # æå¤±é–¢æ•°ï¼ˆchunk-basedå°‚ç”¨ï¼‰
    loss_fn = ProvenceChunkBasedLoss(
        model=model,
        ranking_weight=1.0,
        pruning_weight=0.8,  # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é‡è¦–
        use_teacher_scores=True
    )
    
    # å­¦ç¿’è¨­å®šï¼ˆå¤§å®¹é‡GPUç”¨ï¼‰
    training_args = {
        "output_dir": output_dir,
        "num_epochs": 2,  # æ¤œè¨¼ç”¨ã«çŸ­ç¸®
        "batch_size": 48,  # å¤§å®¹é‡GPUãƒ¡ãƒ¢ãƒªåˆ©ç”¨
        "learning_rate": 2e-5,
        "warmup_ratio": 0.1,
        "weight_decay": 0.01,
        "gradient_accumulation_steps": 1,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º48
        "max_grad_norm": 1.0,
        "logging_steps": 20,  # ã‚ˆã‚Šé »ç¹ã«ãƒ­ã‚°å‡ºåŠ›
        "eval_steps": 200,  # ã‚ˆã‚Šé »ç¹ã«è©•ä¾¡
        "save_steps": 200,  # ã‚ˆã‚Šé »ç¹ã«ä¿å­˜
        "save_total_limit": 3,
        "fp16": True,
        "dataloader_num_workers": 4,  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é«˜é€ŸåŒ–
        "seed": 42
    }
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    print("ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–ä¸­...")
    trainer = ProvenceTrainer(
        model=model,
        train_dataset=train_data,
        eval_dataset=eval_data,
        data_collator=data_collator,
        loss_fn=loss_fn,
        training_args=training_args
    )
    
    # å­¦ç¿’é–‹å§‹
    print(f"ğŸ¯ å­¦ç¿’é–‹å§‹ - ja-minimal ({len(train_data):,} ä»¶)")
    print(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_dir}")
    print(f"âš™ï¸  è¨­å®š: ã‚¨ãƒãƒƒã‚¯æ•°={training_args['num_epochs']}, ãƒãƒƒãƒã‚µã‚¤ã‚º={training_args['batch_size']}, å®ŸåŠ¹BS={training_args['batch_size'] * training_args['gradient_accumulation_steps']}")
    
    try:
        trainer.train()
        print("âœ… å­¦ç¿’å®Œäº†!")
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        final_model_path = os.path.join(output_dir, "final-model")
        model.save_pretrained(final_model_path)
        print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_model_path}")
        
    except Exception as e:
        logger.error(f"âŒ å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    main()