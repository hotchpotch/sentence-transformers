#!/usr/bin/env python3
"""
ja-full ã§å­¦ç¿’ã—ãŸ Provence ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import torch
import numpy as np
from pathlib import Path
from datasets import load_dataset
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from tqdm import tqdm

from sentence_transformers.provence import ProvenceEncoder


def evaluate_model(model_path: str, dataset_subset: str = 'ja-full'):
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ"""
    
    print(f"=== {dataset_subset} ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ ===")
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
    print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰ä¸­...")
    dataset = load_dataset('hotchpotch/wip-query-context-pruner-with-teacher-scores', dataset_subset)
    test_data = dataset['test']
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ä»¶")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
    try:
        model = ProvenceEncoder.from_pretrained(model_path)
        model.eval()
        print("âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    print("ğŸ” è©•ä¾¡å®Ÿè¡Œä¸­...")
    
    # ãƒãƒƒãƒå‡¦ç†ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
    all_pairs = []
    all_pair_labels = []
    all_pair_teacher_scores = []
    
    for item in test_data:
        query = item['query']
        texts = item['texts']
        labels = item['labels']
        teacher_scores = item['teacher_scores_japanese-reranker-xsmall-v2']
        
        for text, label, teacher_score in zip(texts, labels, teacher_scores):
            all_pairs.append((query, text))
            all_pair_labels.append(label)
            all_pair_teacher_scores.append(teacher_score)
    
    print(f"  è©•ä¾¡ãƒšã‚¢æ•°: {len(all_pairs):,}")
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢äºˆæ¸¬
    print("  ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢äºˆæ¸¬ä¸­...")
    ranking_scores = model.predict(all_pairs, batch_size=64, show_progress_bar=True)
    all_predictions = ranking_scores
    all_labels = np.array(all_pair_labels)
    all_teacher_scores = np.array(all_pair_teacher_scores)
    
    compression_results = []
    
    # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è©•ä¾¡ï¼ˆå„é–¾å€¤ã§ï¼‰
    print("  ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è©•ä¾¡ä¸­...")
    for threshold in [0.1, 0.3, 0.5, 0.7]:
        print(f"    é–¾å€¤ {threshold} ã§è©•ä¾¡ä¸­...")
        outputs = model.predict_with_pruning(
            all_pairs,
            batch_size=64,
            pruning_threshold=threshold,
            return_documents=True,
            show_progress_bar=True
        )
        
        for i, output in enumerate(outputs):
            compression_results.append({
                'threshold': threshold,
                'compression_ratio': output.compression_ratio,
                'ranking_score': float(output.ranking_scores),
                'label': all_pair_labels[i],
                'teacher_score': all_pair_teacher_scores[i],
                'is_positive': all_pair_labels[i] == 1
            })
    
    # çµæœåˆ†æ
    print("\nğŸ“Š è©•ä¾¡çµæœ:")
    
    # 1. ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # ãƒã‚¤ãƒŠãƒªåˆ†é¡ã¨ã—ã¦è©•ä¾¡ï¼ˆé–¾å€¤0.5ï¼‰
    binary_preds = (all_predictions > 0.5).astype(int)
    accuracy = accuracy_score(all_labels, binary_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, binary_preds, average='binary', zero_division=0
    )
    
    print(f"\nğŸ¯ ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ€§èƒ½:")
    print(f"  Accuracy: {accuracy:.3f}")
    print(f"  Precision: {precision:.3f}")
    print(f"  Recall: {recall:.3f}")
    print(f"  F1: {f1:.3f}")
    
    # 2. ã‚¹ã‚³ã‚¢ç›¸é–¢
    pos_mask = all_labels == 1
    neg_mask = all_labels == 0
    
    if np.sum(pos_mask) > 0:
        pos_score_mean = np.mean(all_predictions[pos_mask])
        pos_teacher_mean = np.mean(np.array(all_teacher_scores)[pos_mask])
        print(f"  POSäºˆæ¸¬ã‚¹ã‚³ã‚¢å¹³å‡: {pos_score_mean:.3f} (æ•™å¸«: {pos_teacher_mean:.3f})")
    
    if np.sum(neg_mask) > 0:
        neg_score_mean = np.mean(all_predictions[neg_mask])
        neg_teacher_mean = np.mean(np.array(all_teacher_scores)[neg_mask])
        print(f"  NEGäºˆæ¸¬ã‚¹ã‚³ã‚¢å¹³å‡: {neg_score_mean:.3f} (æ•™å¸«: {neg_teacher_mean:.3f})")
    
    # 3. åœ§ç¸®æ€§èƒ½
    print(f"\nâœ‚ï¸  ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½:")
    
    for threshold in [0.1, 0.3, 0.5, 0.7]:
        threshold_results = [r for r in compression_results if r['threshold'] == threshold]
        
        if threshold_results:
            # POS/NEGåˆ¥ã®åœ§ç¸®ç‡
            pos_results = [r for r in threshold_results if r['is_positive']]
            neg_results = [r for r in threshold_results if not r['is_positive']]
            
            all_compression = [r['compression_ratio'] for r in threshold_results]
            pos_compression = [r['compression_ratio'] for r in pos_results] if pos_results else []
            neg_compression = [r['compression_ratio'] for r in neg_results] if neg_results else []
            
            print(f"  é–¾å€¤ {threshold}:")
            print(f"    å…¨ä½“åœ§ç¸®ç‡: {np.mean(all_compression):.1%} Â± {np.std(all_compression):.1%}")
            if pos_compression:
                print(f"    POSåœ§ç¸®ç‡: {np.mean(pos_compression):.1%} Â± {np.std(pos_compression):.1%}")
            if neg_compression:
                print(f"    NEGåœ§ç¸®ç‡: {np.mean(neg_compression):.1%} Â± {np.std(neg_compression):.1%}")
    
    # 4. ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ›
    print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«å‡ºåŠ› (é–¾å€¤0.3):")
    
    # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã‚’æ¢ã™
    positive_items = []
    for item in test_data:
        if item['labels'][0] == 1:  # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆãŒãƒã‚¸ãƒ†ã‚£ãƒ–
            positive_items.append(item)
            if len(positive_items) >= 5:
                break
    
    for i, item in enumerate(positive_items[:5]):
        query = item['query']
        pos_text = item['texts'][0]
        
        try:
            output = model.predict_with_pruning(
                (query, pos_text),
                pruning_threshold=0.3,
                return_documents=True
            )
            
            print(f"\n  ã‚µãƒ³ãƒ—ãƒ« {i + 1} (Positive):")
            print(f"    Query: {query[:100]}...")
            print(f"    å…ƒãƒ†ã‚­ã‚¹ãƒˆé•·: {len(pos_text)} æ–‡å­—")
            if output.pruned_documents and output.pruned_documents[0]:
                pruned_doc = output.pruned_documents[0]
                print(f"    åœ§ç¸®å¾Œé•·: {len(pruned_doc)} æ–‡å­—")
                print(f"    åœ§ç¸®ç‡: {output.compression_ratio:.1%}")
                print(f"    ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢: {float(output.ranking_scores):.3f}")
                print(f"    åœ§ç¸®å¾Œ: {pruned_doc[:300]}{'...' if len(pruned_doc) > 300 else ''}")
            else:
                print(f"    åœ§ç¸®å¾Œ: (ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)")
                print(f"    åœ§ç¸®ç‡: {output.compression_ratio:.1%}")
            
        except Exception as e:
            print(f"    âš ï¸  ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\nâœ… è©•ä¾¡å®Œäº†")


def main():
    # å­¦ç¿’å®Œäº†ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    output_dir = "./outputs/provence-ja-full"
    
    # best modelã¾ãŸã¯æœ€æ–°ã®checkpointã‚’ä½¿ç”¨
    best_model_path = None
    final_model_path = os.path.join(output_dir, "final-model")
    
    # best checkpointã‚’æ¢ã™ï¼ˆç•ªå·ãŒå¤§ãã„æ–¹ã‚’å„ªå…ˆï¼‰
    if os.path.exists(output_dir):
        best_checkpoints = sorted(Path(output_dir).glob("checkpoint-*-best"), 
                                 key=lambda x: int(x.name.split('-')[1]))
        if best_checkpoints:
            best_model_path = str(best_checkpoints[-1])
    
    # è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºå®š
    if best_model_path and os.path.exists(best_model_path):
        model_path = best_model_path
        print(f"ğŸ† Best ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_path}")
    elif os.path.exists(final_model_path):
        model_path = final_model_path
        print(f"ğŸ”š Final ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {model_path}")
    else:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {output_dir}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluate_model(model_path, 'ja-full')


if __name__ == "__main__":
    main()