#!/usr/bin/env python3
"""
çµ±åˆã•ã‚ŒãŸProvenceãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Usage: python train_provence.py --target {minimal|small|full}
"""

import argparse
import os
import logging
from pathlib import Path
from datasets import load_dataset

from sentence_transformers.provence import (
    ProvenceEncoder,
    ProvenceTrainer,
    ProvenceDataCollator,
    ProvenceLoss
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
DATASET_CONFIGS = {
    'minimal': {
        'name': 'ja-minimal',
        'train_samples': None,  # å…¨ã¦ä½¿ç”¨
        'validation_samples': None
    },
    'small': {
        'name': 'ja-small',
        'train_samples': None,
        'validation_samples': None
    },
    'full': {
        'name': 'ja-full',
        'train_samples': None,
        'validation_samples': None
    }
}

# å­¦ç¿’è¨­å®š
TRAINING_CONFIGS = {
    'minimal': {
        'num_epochs': 2,
        'batch_size': 48,
        'learning_rate': 2e-5,
        'warmup_ratio': 0.1,
        'weight_decay': 0.01,
        'gradient_accumulation_steps': 1,
        'max_grad_norm': 1.0,
        'logging_steps': 20,
        'eval_steps': 200,
        'save_steps': 200,
        'save_total_limit': 3,
        'fp16': True,
        'bf16': True,
        'dataloader_num_workers': 4,
        'seed': 42
    },
    'small': {
        'num_epochs': 3,
        'batch_size': 32,
        'learning_rate': 2e-5,
        'warmup_ratio': 0.1,
        'weight_decay': 0.01,
        'gradient_accumulation_steps': 1,
        'max_grad_norm': 1.0,
        'logging_steps': 50,
        'eval_steps': 500,
        'save_steps': 500,
        'save_total_limit': 2,
        'fp16': True,
        'bf16': True,
        'dataloader_num_workers': 4,
        'seed': 42
    },
    'full': {
        'num_epochs': 1,
        'batch_size': 24,
        'learning_rate': 2e-5,
        'warmup_ratio': 0.05,
        'weight_decay': 0.01,
        'gradient_accumulation_steps': 2,
        'max_grad_norm': 1.0,
        'logging_steps': 100,
        'eval_steps': 1000,
        'save_steps': 1000,
        'save_total_limit': 3,
        'fp16': True,
        'bf16': True,
        'dataloader_num_workers': 4,
        'seed': 42
    }
}

# æå¤±é–¢æ•°è¨­å®š
LOSS_CONFIGS = {
    'minimal': {
        'ranking_weight': 1.0,
        'pruning_weight': 0.8,
        'is_regression': True
    },
    'small': {
        'ranking_weight': 1.0,
        'pruning_weight': 0.8,
        'is_regression': True
    },
    'full': {
        'ranking_weight': 1.0,
        'pruning_weight': 0.8,
        'is_regression': True
    }
}


def main():
    parser = argparse.ArgumentParser(description='Train Provence model')
    parser.add_argument(
        '--target', 
        type=str, 
        required=True,
        choices=['minimal', 'small', 'full'],
        help='Target dataset size'
    )
    parser.add_argument(
        '--model_name',
        type=str,
        default='hotchpotch/japanese-reranker-xsmall-v2',
        help='Base model name or path'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default=None,
        help='Output directory (default: outputs/provence-ja-{target}/)'
    )
    parser.add_argument(
        '--max_length',
        type=int,
        default=512,
        help='Maximum sequence length'
    )
    parser.add_argument(
        '--resume_from_checkpoint',
        type=str,
        default=None,
        help='Resume training from checkpoint'
    )
    parser.add_argument(
        '--logging_dir',
        type=str,
        default='./logs',
        help='TensorBoard logging directory'
    )
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    if args.output_dir is None:
        args.output_dir = f'outputs/provence-ja-{args.target}'
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—
    dataset_config = DATASET_CONFIGS[args.target]
    training_config = TRAINING_CONFIGS[args.target]
    loss_config = LOSS_CONFIGS[args.target]
    
    print(f"=== Provence {args.target.upper()} Training ===")
    print(f"Model: {args.model_name}")
    print(f"Dataset: {dataset_config['name']}")
    print(f"Output: {args.output_dir}")
    print(f"Max length: {args.max_length}")
    print("="*50)
    
    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
    model = ProvenceEncoder(
        model_name_or_path=args.model_name,
        max_length=args.max_length,
        pruning_config={
            'num_labels': 2,
            'classifier_dropout': 0.1,
            'sentence_pooling': 'mean',
            'use_weighted_pooling': False,
        }
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    print("ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­...")
    dataset = load_dataset(
        'hotchpotch/wip-query-context-pruner-with-teacher-scores',
        dataset_config['name']
    )
    
    # ãƒ‡ãƒ¼ã‚¿æ•°ã®åˆ¶é™ï¼ˆå¿…è¦ãªå ´åˆï¼‰
    if dataset_config['train_samples']:
        dataset['train'] = dataset['train'].select(range(dataset_config['train_samples']))
    if dataset_config['validation_samples']:
        dataset['validation'] = dataset['validation'].select(range(dataset_config['validation_samples']))
    
    print(f"Training samples: {len(dataset['train'])}")
    print(f"Validation samples: {len(dataset['validation'])}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    data_collator = ProvenceDataCollator(
        tokenizer=model.tokenizer,
        query_column="query",
        texts_column="texts",
        labels_column="labels",
        scores_column="teacher_scores_japanese-reranker-xsmall-v2",
        chunks_pos_column="chunks_pos",
        relevant_chunks_column="relevant_chunks"
    )
    
    # æå¤±é–¢æ•°
    loss_fn = ProvenceLoss(
        model=model,
        ranking_weight=loss_config['ranking_weight'],
        pruning_weight=loss_config['pruning_weight'],
        is_regression=loss_config['is_regression']
    )
    
    # å­¦ç¿’è¨­å®š
    training_args = {
        "output_dir": args.output_dir,
        "num_epochs": training_config['num_epochs'],
        "batch_size": training_config['batch_size'],
        "learning_rate": training_config['learning_rate'],
        "warmup_ratio": training_config['warmup_ratio'],
        "weight_decay": training_config['weight_decay'],
        "gradient_accumulation_steps": training_config['gradient_accumulation_steps'],
        "max_grad_norm": training_config['max_grad_norm'],
        "logging_steps": training_config['logging_steps'],
        "eval_steps": training_config['eval_steps'],
        "save_steps": training_config['save_steps'],
        "save_total_limit": training_config['save_total_limit'],
        "fp16": training_config['fp16'],
        "bf16": training_config['bf16'],
        "dataloader_num_workers": training_config['dataloader_num_workers'],
        "seed": training_config['seed'],
        "logging_dir": args.logging_dir,
        "resume_from_checkpoint": args.resume_from_checkpoint
    }
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–
    print("ğŸš€ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–ä¸­...")
    trainer = ProvenceTrainer(
        model=model,
        train_dataset=dataset['train'],
        eval_dataset=dataset['validation'],
        data_collator=data_collator,
        loss_fn=loss_fn,
        training_args=training_args
    )
    
    # å­¦ç¿’é–‹å§‹
    print("ğŸƒ å­¦ç¿’é–‹å§‹...")
    trainer.train()
    
    # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    if hasattr(trainer, 'best_model_path') and trainer.best_model_path:
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’final-modelã¨ã—ã¦ã‚³ãƒ”ãƒ¼
        import shutil
        print(f"ğŸ’¾ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­: {args.output_dir}/final-model")
        shutil.copytree(trainer.best_model_path, f"{args.output_dir}/final-model")
    else:
        # ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­: {args.output_dir}/final-model")
        model.save_pretrained(f"{args.output_dir}/final-model")
    
    print("âœ… å­¦ç¿’å®Œäº†ï¼")


if __name__ == "__main__":
    main()