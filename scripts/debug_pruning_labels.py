#!/usr/bin/env python3
"""
ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«ç”Ÿæˆã®ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import torch
from datasets import load_dataset
from transformers import AutoTokenizer
from sentence_transformers.provence.data_collator_chunk_based import ProvenceChunkBasedDataCollator
import numpy as np

def main():
    print("=== ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒ‡ãƒãƒƒã‚° ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ­ãƒ¼ãƒ‰
    dataset = load_dataset('hotchpotch/wip-query-context-pruner-with-teacher-scores', 'ja-minimal')
    
    # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
    sample = dataset['train'][0]
    print(f"\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
    print(f"Query: {sample['query']}")
    print(f"Textsæ•°: {len(sample['texts'])}")
    print(f"Labels: {sample['labels']}")
    
    # Tokenizerãƒ­ãƒ¼ãƒ‰
    tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-small')
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼åˆæœŸåŒ–
    collator = ProvenceChunkBasedDataCollator(
        tokenizer=tokenizer,
        max_length=512,
        padding=True,
        truncation=True
    )
    
    # æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã ã‘ã‚’è©³ç´°åˆ†æ
    for text_idx in range(min(3, len(sample['texts']))):
        print(f"\n\n{'='*60}")
        print(f"ğŸ“ Text {text_idx} (label={sample['labels'][text_idx]})")
        print(f"Text: {sample['texts'][text_idx]}")
        print(f"Chunks_pos: {sample['chunks_pos'][text_idx]}")
        print(f"Relevant_chunks: {sample['relevant_chunks'][text_idx]}")
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        encoded = tokenizer(
            sample['query'],
            sample['texts'][text_idx],
            padding=False,
            truncation=True,
            max_length=512,
            return_tensors='pt',
            return_offsets_mapping=True
        )
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ
        tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])
        offsets = encoded['offset_mapping'][0]
        
        print(f"\nğŸ”¤ ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ:")
        # SEPãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚’æ¢ã™
        sep_positions = (encoded['input_ids'][0] == tokenizer.sep_token_id).nonzero(as_tuple=True)[0]
        print(f"SEPãƒˆãƒ¼ã‚¯ãƒ³ä½ç½®: {sep_positions.tolist()}")
        
        if len(sep_positions) >= 3:
            doc_start = sep_positions[1].item() + 1
            doc_end = sep_positions[2].item()
            
            print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†: {doc_start} - {doc_end}")
            print(f"\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒˆãƒ¼ã‚¯ãƒ³:")
            for i in range(doc_start, min(doc_start + 10, doc_end)):  # æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿è¡¨ç¤º
                print(f"  [{i}] '{tokens[i]}' offset={offsets[i].tolist()}")
        
        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆå˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ç”¨ï¼‰
        features = [{
            'query': sample['query'],
            'texts': [sample['texts'][text_idx]],
            'chunks_pos': [sample['chunks_pos'][text_idx]],
            'relevant_chunks': [sample['relevant_chunks'][text_idx]],
            'ranking_labels': [sample['labels'][text_idx]],
            'teacher_scores': [sample['teacher_scores_japanese-reranker-xsmall-v2'][text_idx]]
        }]
        
        # ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼å‘¼ã³å‡ºã—
        batch = collator(features)
        
        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«åˆ†æ
        pruning_labels = batch['labels']['pruning_labels'][0]
        print(f"\nğŸ·ï¸  ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ©ãƒ™ãƒ«çµ±è¨ˆ:")
        print(f"ãƒ©ãƒ™ãƒ«å½¢çŠ¶: {pruning_labels.shape}")
        print(f"1ã®æ•°: {(pruning_labels == 1).sum().item()}")
        print(f"0ã®æ•°: {(pruning_labels == 0).sum().item()}")
        print(f"-100ã®æ•°: {(pruning_labels == -100).sum().item()}")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†ã®ãƒ©ãƒ™ãƒ«ã‚’ç¢ºèª
        if len(sep_positions) >= 3:
            doc_start = sep_positions[1].item() + 1
            doc_end = sep_positions[2].item()
            doc_labels = pruning_labels[doc_start:doc_end]
            print(f"\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆéƒ¨åˆ†ã®ãƒ©ãƒ™ãƒ«:")
            print(f"1ã®æ•°: {(doc_labels == 1).sum().item()}")
            print(f"0ã®æ•°: {(doc_labels == 0).sum().item()}")
            
            # æœ€åˆã®20ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤º
            print(f"\næœ€åˆã®20ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ©ãƒ™ãƒ«:")
            for i in range(min(20, len(doc_labels))):
                if i + doc_start < len(tokens):
                    print(f"  [{i}] '{tokens[i + doc_start]}' = {doc_labels[i].item()}")
        
        # ãƒãƒ£ãƒ³ã‚¯è§£æ
        if sample['relevant_chunks'][text_idx]:
            print(f"\nğŸ“ é–¢é€£ãƒãƒ£ãƒ³ã‚¯è©³ç´°:")
            for chunk_idx in sample['relevant_chunks'][text_idx]:
                if chunk_idx < len(sample['chunks_pos'][text_idx]):
                    start, end = sample['chunks_pos'][text_idx][chunk_idx]
                    chunk_text = sample['texts'][text_idx][start:end]
                    print(f"  Chunk {chunk_idx}: [{start}, {end}]")
                    print(f"  Text: {chunk_text}")


if __name__ == "__main__":
    main()