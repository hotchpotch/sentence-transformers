Title: Provence: efficient and robust context pruning for retrieval-augmented generation

URL Source: http://arxiv.org/html/2501.16214v1

Published Time: Tue, 28 Jan 2025 02:42:47 GMT

Markdown Content:
\correspondingauthor

nadia.chirkova@naverlabs.com, thibault.formal@naverlabs.com \affiliations NAVER LABS Europe, Grenoble, France \website https://huggingface.co/naver/provence-reranker-debertav3-v1 \websiteref[](https://huggingface.co/naver/provence-reranker-debertav3-v1)\teaserfig![Image 1: [Uncaptioned image]](https://arxiv.org/html/2501.16214v1/extracted/6159612/Provence_ill.png)

###### Abstract

Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are however limited, and do not provide a universal model that would be both efficient and robust in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work.

1 Introduction
--------------

Retrieval-Augmented Generation (RAG) has become a widely-used paradigm for improving factuality, attribution, and adaptability of Large Language Models (LLMs) (Das et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib13); Asai et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib3); Seo et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib46); Lewis et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib30); Mallen et al., [2023a](https://arxiv.org/html/2501.16214v1#bib.bib35); Min et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib37)). Augmenting a given user‚Äôs query with retrieved relevant contexts helps to avoid the generation of untruthful information and enables the provision of references used to generate the answer. Furthermore, using a domain-specific datastore may enable access and reasoning over a previously unknown knowledge ‚Äì without fine-tuning the LLM. One additional advantage of the RAG approach is the easy plug-and-play architecture ([LangChain,](https://arxiv.org/html/2501.16214v1#bib.bib27)): practitioners may choose components (retrievers, generator LLMs, context granularity etc.) which best suit their particular cases to maximize the final performance.

![Image 2: Refer to caption](https://arxiv.org/html/2501.16214v1/x1.png)

Figure 1: Illustration of inference (left) and training (right) of Provence.

Table 1: Analysis of existing approaches for context pruning. Violet / Orange highlight practical / less-practical solutions. 

At the same time, the use of RAG adds computational overhead due to both retrieval latency and the increased input length for the LLMs. It may also propagate irrelevant information present in retrieved contexts into generated responses. These issues can be solved by developing more efficient and robust LLMs ‚Äì either by making architectural changes to process long contexts more efficiently (Nawrot et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib38); Dao, [2024](https://arxiv.org/html/2501.16214v1#bib.bib12); Chevalier et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib7); Louis et al., [2025](https://arxiv.org/html/2501.16214v1#bib.bib34)) or increasing the diversity of the tuning data to improve processing of irrelevant contexts (Lin et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib33)). However, tuning the LLM can be highly resource-consuming, or even impossible to apply for proprietary (closed) LLMs. An alternative solution consists in pruning retrieved contexts by removing context parts irrelevant to the user‚Äôs query ‚Äì which reduces context lengths and therefore speeds up generation. Such context pruning module can be used in a plug-and-play manner with any generator LLM, featuring both easy use and better transparency in the RAG pipeline.

Despite initial efforts on developing context pruners for RAG, none of the existing solutions provide a model ready to be used out-of-the-box in practice. First, many approaches are designed for a simplified setting, e.g., with the assumption that only one sentence per context is relevant to the input query (Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50); Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)), or that the compression ratio is fixed (Jiang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib22); Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)). However, in practice contexts may contain various portions of relevant information, from empty to full relevant context, and pruners should detect it in an adaptable fashion. Second, many works introduce context pruners that are not efficient enough to be used in practice. This includes using billion-sized LLMs as base models for pruners (Jiang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib23); Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42); Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50)), or designing abstractive context compressors which require sequential autoregressive generation of the final context (Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50); Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)). We argue that a more practical and efficient setting consists in fine-tuning a small-size model such as DeBERTa(He et al., [2021b](https://arxiv.org/html/2501.16214v1#bib.bib17), [a](https://arxiv.org/html/2501.16214v1#bib.bib16)), as an extractive pruner, i.e., with a lightweight prediction head for selecting relevant context parts. Third, most of the existing works train context pruners for each dataset individually and do not target nor test pruners robustness to various data domains.

Table[1](https://arxiv.org/html/2501.16214v1#S1.T1 "Table 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") summarizes the properties of various existing methods along specified dimensions and shows that none of them satisfy all listed criteria. The table also includes a dimension of pruning granularity, i.e., token-level vs sentence-level pruning. In this work, we focus on query-dependent sentence-level pruning, which prunes out semantic units (sentences) that are deemed not relevant to generate the answer. An alternative approach is token-level pruning which prunes out low-level grammatical units such as articles or interjections, usually in a query-independent fashion. The two approaches are orthogonal and could potentially be combined.

To address listed limitations, we introduce `Provence` (Pruning and Reranking Of retrieVEd relevaNt ContExt), an approach for training an adaptable, efficient and robust sentence-level context pruner for Question Answering, which can be used out-of-the-box across various domains and settings. To achieve this, we formulate context pruning as binary sequence labeling so that the binary mask predicted by the pruner determines sentences (from zero to all) which are relevant to the query, and train our pruner from a lightweight DeBERTa model on diverse data. Furthermore, we notice that context pruning and reranking (i.e., the second step in effective retrieval pipelines) bear a strong resemblance. We therefore propose to unify these two models into a single one, completely eliminating the cost of context pruning in the RAG pipeline.

More specifically, our contributions are as follows:

*   ‚Ä¢We propose an approach for training an adaptable, robust, and efficient context pruner for QA ‚Äì and release our trained model. Three key ingredients of our approach are formulating context pruning as sequence labeling, unifying context pruning and reranking in a single model, and training on diverse data. 
*   ‚Ä¢We test `Provence` on various QA domains and show its out-of-the-box applicability to prune contexts with negligible to no drop in performance and at almost no cost, substantially outperforming baseline approaches. We also demonstrate `Provence` capabilities in detecting the number of relevant sentences at any positions in the context and robustness to various context lengths. 
*   ‚Ä¢We conduct multiple ablations to demonstrate which techniques are essential for training robust context pruners, to provide insights for future context pruners development. 

#### Definitions.

A typical RAG pipeline consists of (0) a user‚Äôs question, or query; (1) a datastore, i.e., a collection of documents (pieces of text) to be retrieved from, (2) an efficient retriever which enables fast retrieval from a large datastore (typically a dual-encoder model, where queries and passages are encoded independently), (3) a more expensive cross-encoder reranker which further reduces and reorders a set of retrieved passages (cross-encoding means encoding a passage together with a query); and (4) a generator LLM which outputs the final response based on the user‚Äôs query and the relevant passages. Such a pipeline can be represented as retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>generate. Context pruning can be incorporated before generation, i.e., retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>prune>>much-greater-than>>>>generate. In our work, we also propose to incorporate context pruning into reranking, an essential and already present component in RAG(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)): retrieve>>much-greater-than>>>>rerank+prune>>much-greater-than>>>>generate. This enables context pruning at almost zero cost.

2 Related work
--------------

Context pruning. RECOMP(Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)) focuses on context pruning for RAG and proposes both extractive and abstractive context pruners. The extractive RECOMP approach independently encodes sentences in the context and then selects top sentences with embeddings closest to the query embedding. Such an approach limits context understanding, due to independent processing of both sentences and queries. The method also requires specifying the amount of sentences to keep as a hyperparameter ‚Äì which is usually unknown in practice and should depend on each particular passage. The abstractive RECOMP summarizes key information from the passage relevant to the query (including zero relevant information) by training on silver summaries generated by GPT-3.5. However, it requires inefficient autoregressive generation of the final context, and can eventually hallucinate facts not present in the input context. FilCo(Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50)) similarly proposes to generate contexts autoregressively but is trained on extractive targets, i.e., one sentence from the context selected by one of three criteria. The drawbacks are again inefficiency and the simplified assumption of one relevant sentence per context. A recent approach, COMPACT(Yoon et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib54)), also proposes to generate filtered contexts autoregressively ‚Äì hence inefficiently ‚Äì and introduces an iterative approach for gradually updating the relevant context after processing a new portion of retrieved passages. In contrast to all listed efforts, `Provence` dynamically detects the amount of relevant information in the context ‚Äì from zero to all sentences ‚Äì in an extractive and efficient way. Furthermore, we propose a novel approach of integrating context pruning into a reranker.

Concurrently to our work, DSLR (Hwang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib20)) performs extractive sentence-level pruning, by encoding sentences one-by-one, together with the query, using existing rerankers. Similarly to Provence, DSLR keeps sentences with scores higher than a threshold and preserves the original order of sentences. However, in contrast to Provence, DSLR is not capable of keeping groups of semantically connected sentences, due to independent sentence processing.

An orthogonal line of work proposes extractive token-level pruners. LLMLingua(Jiang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib22)) and Selective Context (Li et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib31)) use LLMs to remove tokens with high generation probabilities, independently of the query. LLMLingua2(Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)) is a small BERT-based model finetuned to eliminate redundant tokens, also independently of the query. LongLLMLingua(Jiang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib23)) proposes query-dependent LLM-based token pruning based on contrastive perplexity. Listed approaches remove tokens in a way that it does not break context understanding for the LLM ‚Äì hence they are not capable of removing semantic parts of the context. LLMLingua models also have many hyperparameters in the interface which are hard to tune in practice. These approaches can however also be combined with sentence-level pruning.

Retrieval granularity. Alternatively to context pruning, one can reformulate datastore content into atomic units, e.g., propositions as in Dense-X retrieval Chen et al. ([2024c](https://arxiv.org/html/2501.16214v1#bib.bib6)) or decontextualized sentences(Choi et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib8)). Such preprocessing is expensive and can lead to some information loss.

Passage filtering. Another related ‚Äì and orthogonal ‚Äì line of works focuses on filtering entire passages if they are deemed irrelevant for a given question; such an approach can be straightforwardly combined with `Provence`. A simple method consists in introducing a threshold on the (re)ranking score. LongLLMLingua reranks passages based on the probability of a question given the passage. (Yoran et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib55)) use natural language inference models to filter out passages that do not entail question-answer pairs, but report that this approach sometimes filters out relevant passages too.

Improving context processing in LLMs. While context pruners aim to remove context parts irrelevant to the user‚Äôs query, another line of work aims to process contexts more efficiently and effectively in LLMs. Efficient context processing could be achieved through efficient attention implementations(Dao, [2024](https://arxiv.org/html/2501.16214v1#bib.bib12); Anagnostidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib2)), KV cache compression(Nawrot et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib38)), encoding retrieved passages in parallel(Zhu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib56)), or compressing contexts into one or more context embeddings(Chevalier et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib7); Ge et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib15); Rau et al., [2024b](https://arxiv.org/html/2501.16214v1#bib.bib45); Louis et al., [2025](https://arxiv.org/html/2501.16214v1#bib.bib34)). Other works aim to make LLMs more robust, by exposing them to noisy contexts during training or finetuning(Izacard et al., [2022](https://arxiv.org/html/2501.16214v1#bib.bib21); Lin et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib33)). All such approaches usually require LLM adaptation which may complicate application to an arbitrary picked LLM.

3 Provence
----------

The high-level overview of our proposed approach is illustrated in Figure[1](https://arxiv.org/html/2501.16214v1#S1.F1 "Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). Our first contribution is to pose the context pruning problem as a sequence labeling task. We fine-tune a DeBERTa model to encode the query‚Äìcontext pair and output binary masks which are used to filter out irrelevant context parts. The labels for training are generated by LLama-3-8B-Instruct(AI@Meta, [2024](https://arxiv.org/html/2501.16214v1#bib.bib1)); we call them silver labels since they are generated automatically. Such an approach solves several limitations of existing context pruners: (1) by construction, the model is able to deal with varying noise in contexts and select an appropriate pruning ratio; (2) queries are encoded together with context sentences (cross-encoding), providing richer representations ‚Äì compared for instance to extractive RECOMP which encodes query and context sentences independently; (3) using a lightweight encoder makes our approach more efficient than LLM-based or abstractive methods.

Our second contribution consists in unifying reranking and context pruning ‚Äì instead of considering these steps as distinct in the RAG pipeline. In `Provence`, reranking and pruning can be done in a single forward step, thus eliminating the computational overhead due to context pruning ‚Äì making `Provence` almost ‚Äúfree‚Äù.

Training data. Our approach requires a set of training questions and a retrieval datastore. Speficially, we rely on the train set of the MS MARCO document ranking collection which includes 370 k ùëò k italic_k queries(Nguyen et al., [2016](https://arxiv.org/html/2501.16214v1#bib.bib40)). The MS MARCO collection is a domain-diverse datastore of 3.2‚Å¢M 3.2 ùëÄ 3.2M 3.2 italic_M documents crawled from the Web ‚Äì which is required for the final model‚Äôs robustness to various domains ‚Äì and is often used to train retrievers and rerankers. We also consider the train set of Natural Questions which contains 87 k ùëò k italic_k queries Kwiatkowski et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib25)).

Data processing. We create a retrieval datastore by splitting MS MARCO documents into passages consisting of N ùëÅ N italic_N consecutive sentences ‚Äì N ùëÅ N italic_N being a random integer ‚àà1..10 absent 1..10\in 1..10‚àà 1..10. This is to enable the pruner‚Äôs robustness to variable retrieved context lengths. We also prepend page titles to each passage. For each question, we retrieve top-5 relevant passages using a strong retrieval pipeline(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)) consisting of a SPLADE-v3 retriever(Lassance et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib29)) and a DeBERTa-v3 reranker(Lassance & Clinchant, [2023](https://arxiv.org/html/2501.16214v1#bib.bib28)). The resulting set of retrieved passages is naturally diverse w.r.t. relevance or irrelevance to the question, due to imperfections in retrieval.

Silver labels generation. Given a question and a retrieved passage (context), we split the passage into sentences 1 1 1 using the nltk.sent_tokenize function: [https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html) and prompt Llama-3-8B-Instruct to select sentences relevant to the given question. One approach would be to use a straightforward prompt such as ‚ÄúOutput indexes of sentences relevant to the given question‚Äù. However, we decided to utilize the strong LLMs‚Äô capabilities of actually answering questions while citing relevant context sentences. We therefore instruct the LLM to answer the given question using only information provided in the given context, and output ‚ÄúNo answer‚Äù in case no relevant information is provided. We also specify the easy-to-parse citation format `[i]` and number sentences with the same marker in the context. Our prompt can be found in Appendix ‚Äì Table[6](https://arxiv.org/html/2501.16214v1#A2.T6 "Table 6 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"); we use greedy decoding and parse cited sentences using regular expressions. We also compare different prompting strategies in the ablation study.

We found that Llama-3-8B is well capable of answering only based on a given context in most cases and of outputting a citation ‚àº90%similar-to absent percent 90\sim 90\%‚àº 90 % of the time. We filter out cases when no citations are produced and "No answer" is not present in the LLM‚Äôs output, as these are the cases when the context actually contains relevant information but the LLM ‚Äúforgot‚Äù to cite it. The final labels distribution (number of selected sentences per context, their positions) is shown in Appendix ‚Äì Figure[5](https://arxiv.org/html/2501.16214v1#A2.F5 "Figure 5 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Training of Provence. Our context pruner receives as input the concatenation of a question and a retrieved context, and outputs per-token binary labels denoting whether each token (defined by the pretrained model‚Äôs tokenizer) should be included in the selected context. In Section[4.4](https://arxiv.org/html/2501.16214v1#S4.SS4 "4.4 Ablations ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Ablations), we also consider an approach where a special token is inserted at the beginning of each sentence, and labels are predicted per-sentence based on the representations of those tokens. We train `Provence` as a binary per-token classifier with ground truth labels coming from the silver data labeling, and the model can be used as a standalone pruner, i.e., retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>Provence (standalone)>>much-greater-than>>>>generate.

Unifying compression and reranking. We note that cross-encoder rerankers(Nogueira & Cho, [2020](https://arxiv.org/html/2501.16214v1#bib.bib41)) share both the same architecture and inputs (pairs of question‚Äìpassages) as `Provence`. Additionally, the task of context pruning (selecting parts of contexts that are useful for generating the answer to the question) intrinsically bears similarity with re-ranking (estimating the relevance of a context w.r.t. the question) ‚Äì and we hypothesize the possibility of knowledge transfer between these two related tasks. We therefore propose to unify both approaches in a single model, with two different task heads. More specifically, the reranking head outputs a scalar prediction for the `BOS` token while the pruning head outputs per-token predictions for the passage tokens, as illustrated in Figure[1](https://arxiv.org/html/2501.16214v1#S1.F1 "Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). To ease training, we propose to further fine-tune a pretrained reranker on our labeling objective, while adding a ranking ‚Äúregularizer‚Äù to preserve initial reranking capabilities. The regularizer is a Mean Squared Error loss on the reranking scores from the initial reranker. This can be viewed as a straightforward pointwise score distillation process, where the initial model serves as the teacher ‚Äì a method that has demonstrated great effectiveness in Information Retrieval Hofst√§tter et al. ([2021](https://arxiv.org/html/2501.16214v1#bib.bib18)). The final loss function is as follows:

‚Ñí=‚àën=1 N{‚àëk=1 L n log‚Å°P‚Å¢(y n,k|z n,k)+Œª‚Å¢(s n‚àíz n,0)2}z n=Provence‚Å¢(x n)‚Ñí superscript subscript ùëõ 1 ùëÅ superscript subscript ùëò 1 subscript ùêø ùëõ ùëÉ conditional subscript ùë¶ ùëõ ùëò subscript ùëß ùëõ ùëò ùúÜ superscript subscript ùë† ùëõ subscript ùëß ùëõ 0 2 subscript ùëß ùëõ Provence subscript ùë• ùëõ\begin{split}\mathcal{L}=\sum_{n=1}^{N}\biggl{\{}\sum_{k=1}^{L_{n}}\log P(y_{n% ,k}|z_{n,k})+\lambda\bigl{(}s_{n}-z_{n,0}\bigr{)}^{2}\biggr{\}}\\ z_{n}=\text{Provence}(x_{n})\end{split}start_ROW start_CELL caligraphic_L = ‚àë start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT { ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT roman_log italic_P ( italic_y start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT ) + italic_Œª ( italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT - italic_z start_POSTSUBSCRIPT italic_n , 0 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = Provence ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_CELL end_ROW(1)

where N ùëÅ N italic_N is the number of datapoints (query‚Äìpassage pairs), x n subscript ùë• ùëõ x_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n+1 subscript ùêø ùëõ 1 L_{n}+1 italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 input tokens (concatenated query, passage and `BOS` at the 0-t‚Å¢h ùë° ‚Ñé th italic_t italic_h position), z n subscript ùëß ùëõ z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n+1 subscript ùêø ùëõ 1 L_{n}+1 italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 predictions output by the model, y n subscript ùë¶ ùëõ y_{n}italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n subscript ùêø ùëõ L_{n}italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT target binary labels for context pruning, s n subscript ùë† ùëõ s_{n}italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is the teacher score (initial reranker), z n,0 subscript ùëß ùëõ 0 z_{n,0}italic_z start_POSTSUBSCRIPT italic_n , 0 end_POSTSUBSCRIPT is the ranking score predicted from the `BOS` representation.

In the case of the unified model, re-ranking and context pruning need a single forward step from the encoder, i.e., retrieve>>much-greater-than>>>>Provence (w/ re-ranking)>>much-greater-than>>>>generate ‚Äì making context pruning almost free in terms of execution time.

Inference with Provence. At inference, we feed a concatenation of a question and a retrieved passage through `Provence`, which outputs probabilities of including each token in the final context, as well as the passage score in the case of the unified model. We simply use a threshold T ùëá T italic_T to binarize the token probabilites (keep or not) ‚Äì which has a direct effect on the compression rate. As shown in the experiments Section, the choice of a threshold is generally transferable across various datasets, making the model flexible to be used out-of-the box in various QA applications 2 2 2 Note that tuning the threshold per dataset could of course further improve results..

We note that our model outputs token-level predictions despite the sentence-level labeling task. We found that probabilities of including tokens into the final context are naturally clustered on the sentence level ‚Äì see example in Appendix Figure[6](https://arxiv.org/html/2501.16214v1#A2.F6 "Figure 6 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") ‚Äì due to the sentence-level targets used in training. However, in rare cases we could still have partial sentences being selected. To avoid this phenomenon, we apply a ‚Äúsentence rounding‚Äù procedure: for each sentence, we check the ratio of kept tokens (predicted label=1 absent 1=1= 1), and select the entire sentence only if it is higher than 0.5 0.5 0.5 0.5.

![Image 3: Refer to caption](https://arxiv.org/html/2501.16214v1/x2.png)

Figure 2: Main results for various QA domains, comparing Provence and baseline models. Generator: LLama-2-7B, retriever: SPLADE-v3, reranker: DeBERTa-v3 (or Provence in the unified setting). Plot titles denote ‚ÄúDataset name (datastore type)‚Äù. x ùë• x italic_x-axis denotes QA performance evaluated with LLM-as-a-judge; y ùë¶ y italic_y-axis denotes the context compression ratio. For both metrics, the higher the better: the best model would be closest to the top right corner. Numerical scores are presented in App. Tables[8](https://arxiv.org/html/2501.16214v1#A2.T8 "Table 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")‚Äì[9](https://arxiv.org/html/2501.16214v1#A2.T9 "Table 9 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). Main conclusion: Provence consistently lies on the Pareto front.

4 Experiments
-------------

### 4.1 Experimental setup

Provence training details. We train `Provence` on the data described in Section[3](https://arxiv.org/html/2501.16214v1#S3 "3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), using PyTorch(Paszke et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib43)) and HuggingFace transformers(Wolf et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib51)). We use DeBERTa-v3(He et al., [2021a](https://arxiv.org/html/2501.16214v1#bib.bib16)) as our pretrained model for training the standalone `Provence`. For the unified approach, we start training from an already trained cross-encoder, also based on DeBERTa-v3(Lassance & Clinchant, [2023](https://arxiv.org/html/2501.16214v1#bib.bib28)). Note that in the latter, we initialize the ranking head from its fine-tuned version, and train the separate pruning head from scratch.

After preliminary experiments, we set the learning rate to 3√ó10‚àí6 3 superscript 10 6 3\times 10^{-6}3 √ó 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT, the batch size to 48 and train models for one epoch. For joint training, there is a slight trade-off between pruning and reranking. We set the reranking regularization coefficient Œª ùúÜ\lambda italic_Œª to 0.05 0.05 0.05 0.05, chosen as the minimal value that does not substantially degrade reranking performance on the MS MARCO development set.

Table 2: Time/MFLOPS required for context pruning, 50 samples, with batch size set 1 (1 sample consists of a query and top retrieved documents). Top-5 retrieved documents.

Table 3: Speed up in generation due to compression (Provence, 49% compression). Batch sizes 1 or 256. 

Evaluation datasets. We test `Provence` on a diverse set of QA datasets. First, we consider commonly used datasets relying on Wikipedia datastore: Natural Questions(Kwiatkowski et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib25)), TyDi QA(Clark et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib9)), PopQA(Mallen et al., [2023b](https://arxiv.org/html/2501.16214v1#bib.bib36)) (all single-hop questions), and HotpotQA(Yang et al., [2018](https://arxiv.org/html/2501.16214v1#bib.bib53)) (multi-hop questions). Second, we consider datasets with datastores from various domains: BioASQ(Nentidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib39)) (biomedical questions with Pubmed as a datastore), SyllabusQA(Fernandez et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib14)) (questions about educational course logistics, with courses syllabus as a datastore); and RGB Chen et al. ([2024b](https://arxiv.org/html/2501.16214v1#bib.bib5)) (questions about news with Google-searched news as contexts). Further details can be found in Appendix[A](https://arxiv.org/html/2501.16214v1#A1 "Appendix A Data ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Evaluation settings. We conduct experiments using BERGEN(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)), a benchmarking library for RAG, using the recommended experimental setting. For each query, we retrieve top-5 relevant passages using a strong and robust retrieval pipeline: SPLADE-v3 >>much-greater-than>>>> DeBERTa-v3 reranker (except for RGB, for which Google-searched passages are already provided). We then pass queries prepended with relevant document (full length or pruned) into LLama-2-7B-chat(Touvron et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib49))3 3 3 For main experiments, we chose a ‚Äúweaker‚Äù generator which relies more on contexts, to create a more challenging setting for context pruners; results with stronger generators are reported in Appendix ‚Äì Figure[8](https://arxiv.org/html/2501.16214v1#A2.F8 "Figure 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). to generate answers; other retrieval-generator settings are further reported in Appendix. Each evaluation dataset comes with short keyword answers, which we use to evaluate responses using LLM-based evaluation (LLMeval in Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)); match-based metrics are also reported in Appendix. We additionally measure compression as a portion of the context which was pruned out.

We compare `Provence` to publicly available context pruning models listed in Table[1](https://arxiv.org/html/2501.16214v1#S1.T1 "Table 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), except LLMLingua and Selective Context which were shown to underperform LLMLingua2(Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)). For all context pruners (except abstractive RECOMP for which it is not available), we enforce the selection of the first (title) passage sentence, to avoid ambiguity in understanding the context by the generator. For extractive RECOMP, we use the model trained on NQ, consider using top-1/2/3 sentences, and prepend the passage title to each sentence. For the LLMLingua family, we vary the compression rate in {0.25,0.5,0.75}0.25 0.5 0.75\{0.25,0.5,0.75\}{ 0.25 , 0.5 , 0.75 } and use code provided on the official repository 4 4 4[https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua). We use the XLM-RoBERTa model for LLMLingua2. For `Provence`, we use T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1 and T=0.5 ùëá 0.5 T=0.5 italic_T = 0.5. We also compare our method to DSLR based on the same reranker as ours, i.e, DeBERTa-v3.

### 4.2 Main results

Context pruners are often only tested on limited domain data, e.g., with Wikipedia datastore, and an important aspect of our work is evaluating context pruning on a series of QA domains. Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports the trade-off between compression (efficiency) and LLM-evaluated performance (quality), for various QA datasets and context pruning methods. We choose to report a figure per dataset to better assess the Pareto front of existing solutions, rather than comparing methods with different compression rates in the same table. Figure[7](https://arxiv.org/html/2501.16214v1#A2.F7 "Figure 7 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") in Appendix further reports similar results with match-based metric, and Appendix Tables[11](https://arxiv.org/html/2501.16214v1#A2.T11 "Table 11 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")‚Äì[13](https://arxiv.org/html/2501.16214v1#A2.T13 "Table 13 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") show examples of context pruning with various methods.

First, we observe that `Provence` achieves the highest performance across pruning methods, for similar compression ratios. Second, it is noteworthy that `Provence` outperforms methods requiring more computations such as LLMLingua models, showing that efficiency is not traded for effectiveness. Furthermore, `Provence` is the only method capable of achieving high compression levels without (or with negligible) performance drops, on all datasets. Moreover, for some datasets, e.g., PopQA, pruning with `Provence` leads to performance improvements due to noise filtering.

![Image 4: Refer to caption](https://arxiv.org/html/2501.16214v1/x3.png)

![Image 5: Refer to caption](https://arxiv.org/html/2501.16214v1/x4.png)

![Image 6: Refer to caption](https://arxiv.org/html/2501.16214v1/x5.png)

Figure 3: Analyses. (Left) Needle-in-the-haystack test allowing the control of the position of the ground truth sentence(s) in the context. (Middle) Comparison of the number of selected sentences by the silver predictor (LLaMA-3-8B-Instruct) and Provence. Heatmaps are normalized by rows: a cell in position (i,j)ùëñ ùëó(i,j)( italic_i , italic_j ) indicates which percentage of contexts that were pruned into i ùëñ i italic_i sentences by the silver predictor, were pruned into j ùëó j italic_j sentences by Provence. (Right) Testing Provence in settings with different context lengths. All experiments are done with unified Provence, T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1.

The effect of threshold. An important aspect in the out-of-the-box applicability of context pruners is how much effort is needed to select the suitable values of hyperparameters. For `Provence`, it only consists in setting the pruning threshold T ùëá T italic_T. In Figure [2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (for which T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1 and T=0.5 ùëá 0.5 T=0.5 italic_T = 0.5), we observe that `Provence` pruning ratio automatically varies from 50% to 80%, depending on the dataset, which demonstrates that the same values for T ùëá T italic_T work well for all considered domains ‚Äì making Provence robust to the choice of hyperparameters. If necessary, users can still tune it further for their datasets and/or needs. We note that some models specify the desired compression ratio as a hyperparameter, e.g., LLMLingua models or extractive RECOMP (through top-N ùëÅ N italic_N sentences). While it may seem convenient to estimate inference cost, the ‚Äúoptimal‚Äù compression ratio (without losing performance) is specific to each particular question-context pair. Thus, using a threshold as a hyperparameter is more appropriate for this task. We also experimented with specifying a threshold in extractive RECOMP (shown on the same plot) and found that it often leads to lower performance (compared to top-N ùëÅ N italic_N). The reason is that different queries have different ranges of similarity scores.

Efficiency. We compare `Provence` with other pruning methods in terms of efficiency. Table [2](https://arxiv.org/html/2501.16214v1#S4.T2 "Table 2 ‚Ä£ 4.1 Experimental setup ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports compression time and MFLOPS 5 5 5 We use the PyTorch profiler to report FLOPS required by each pruner. required by different pruning methods. As expected, LongLLMLingua (based on LLama-2-7B-chat) is the slowest context pruner. RECOMP abstr. requires less MFLOPS compared to `Provence`, but its autoregressive nature makes it slower in practice 6 6 6 This highlights the fact that MFLOPS do not always align with real inference time, due to different architectural choices.. Note that in the case of the unified model, pruning is almost free ‚Äì as it‚Äôs part of the re-ranking step. Table [3](https://arxiv.org/html/2501.16214v1#S4.T3 "Table 3 ‚Ä£ 4.1 Experimental setup ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports speed-up gains due to compression with `Provence` model (‚àºsimilar-to\sim‚àº 50% compression rate). All runs were performed on single Tesla V100-SXM2-32GB GPU with vllm Kwon et al. ([2023](https://arxiv.org/html/2501.16214v1#bib.bib26)). With large batch sizes, we systematically observe 2√ó\times√ó speed-ups at inference, while smaller batch sizes lead to lower gains (especially for smaller models). We assume this is mostly due to the CPU/GPU communication bottleneck, which masks inference gains due to compression.

### 4.3 Analysis

In this Section, we conduct a more fine-grained evaluation to better understand the properties of `Provence`.

Robustness to the position of relevant information in the context. We design a needle-in-the-haystack experiment which allows us to check the performance of `Provence` on a simple toy example and to evaluate its robustness w.r.t. the position of the relevant information in the input context. We write 5 questions and answers 7 7 7 Example: ‚ÄúWhich library was used in the experiments?‚Äù, answer: ‚ÄúExperiments were conducted using the Bergen library‚Äù. Example reformulation into a 2-sentence answer: ‚ÄúExperiments were conducted using a library. Its name is Bergen.‚Äù, and insert answers (‚Äúneedles‚Äù) at random positions between sentences, in a subset of 100 passages sampled from the Wikipedia datastore. Ideally, `Provence` should only select the ‚Äúneedle‚Äù sentences and filter out all other sentences in contexts. We plot the number of selected sentences and percentage of cases when the pruned context contains the ‚Äúneedle‚Äù (Figure [3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), (Left)). We consider two settings: with 1- and 2-sentence ‚Äúneedles‚Äù. We observe that `Provence` correctly selects ‚Äúneedle‚Äù sentence(s) in most cases, except at leftmost and rightmost positions.8 8 8 The reason for the drops in the left-most and right-most positions is that training data has little examples of the corresponding types of relevant sentences, see e.g., statistics for the rightmost position in the App. Figure 5, (Right). We plan to work on further improving processing of these positions in future work. In most cases `Provence` does not select any irrelevant sentences. The results are similar for both simpler (1-sentence) and harder (2-sentence) ‚Äúneedles‚Äù showing `Provence`‚Äôs flexibility in detecting the number of relevant sentences, discussed below in more details.

Adaptability to the variable number of relevant sentences. To evaluate the capability of `Provence` to dynamically detect the number of relevant sentences in the context, we compare the number of sentences L ùêø L italic_L selected by `Provence` and by a silver oracle, for question-context examples from various datasets. A silver oracle is easy to construct for L=0 ùêø 0 L=0 italic_L = 0, by pairing questions with randomly sampled contexts. For L‚©æ1 ùêø 1 L\geqslant 1 italic_L ‚©æ 1, we use the labeling produced by Llama-3-8B-Instruct. Figure[3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Middle) shows that the number of relevant sentences detected by `Provence` is close to the silver oracle value in most cases, for all considered datasets. In contrast, extractive RECOMP would always select a prespecified number of sentences.

![Image 7: Refer to caption](https://arxiv.org/html/2501.16214v1/x6.png)

Figure 4: Ablation results. All models are single-component modifications of the anchor model, which is a base-size model, trained on NQ data, with the answer oracle and token-level labeling. Numeric scores for this figure are duplicated in Appendix Table[10](https://arxiv.org/html/2501.16214v1#A2.T10 "Table 10 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), and results with match-based metrics are presented in Appendix ‚Äì Figure[11](https://arxiv.org/html/2501.16214v1#A2.F11 "Figure 11 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Robustness w.r.t. context granularity. Figure [3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Right) shows `Provence` performance for two datasets, with Wikipedia datastores made of contexts of various granularity. Here, each considered datastore is produced by splitting Wikipedia pages into chunks of N ùëÅ N italic_N sentences, N‚àà{2,6,10}ùëÅ 2 6 10 N\in\{2,6,10\}italic_N ‚àà { 2 , 6 , 10 }, or 100 words, and prepending the page title to each chunk. `Provence` shows high performance in all cases ‚Äì the performance with pruned contexts being close to the performance obtained using original contexts. As could be expected, the compression ratio is higher for longer contexts.

Reranking effectiveness. Table[4](https://arxiv.org/html/2501.16214v1#S4.T4 "Table 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") compares reranking performance between our reranking baseline and unified `Provence` ‚Äì whose training starts from the former. We can see that our joint training procedure (on both pruning and ranking tasks) makes it possible to learn a context pruner that preserves initial reranking capabilities. We further include as a comparison point results from a model trained in similar conditions on NQ. Overall, results are similar ‚Äì further highlighting the robustness of Provence w.r.t. training data. We further discuss such aspects in Section[4.4](https://arxiv.org/html/2501.16214v1#S4.SS4 "4.4 Ablations ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Ablations).

Applicability in different settings. Figure[8](https://arxiv.org/html/2501.16214v1#A2.F8 "Figure 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (App.) demonstrates the applicability of `Provence` in variable retrieval-generator settings ‚Äì achieving similar results as the ones reported in Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Table 4: Effectiveness of reranking top-50 documents retrieved by SPLADE-v3. DeBERTa-v3 is the ‚Äúbaseline‚Äù (initialization point for Provence, which we aim to preserve performance). We report the R@5 on two RAG datasets (NQ and HotpotQA), MRR@10 on MS MARCO passages (dev set), nDCG@10 on TREC DL‚Äô19(Craswell et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib10)), and mean nDCG@10 on the 13 open datasets from the BEIR benchmark(Thakur et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib48)) ‚Äì Table[7](https://arxiv.org/html/2501.16214v1#A2.T7 "Table 7 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") in Appendix reports the full results.

### 4.4 Ablations

In this Section we analyze various design choices made in `Provence` development, to provide insights into training context pruners for future works (results shown in Figure[4](https://arxiv.org/html/2501.16214v1#S4.F4 "Figure 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")). All models in this section are standalone context pruners, trained with the same amount of parameter updates.

#### Model size.

We first observe that DeBERTa-large slighly increases the compression rate ‚Äì when comprared to DeBERTa-base. All other ablations are tuned from a DeBERTa-base model, for efficiency reasons. Note that the final `Provence` is trained from a DeBERTa-large model (or its equivalent reranker).

Data mixtures. We compare training on NQ (87 k ùëò k italic_k queries), MS MARCO downsampled to the same size, and full MS MARCO (370 k ùëò k italic_k queries). We observe that using the MS MARCO type of data performs similarly to training on NQ ‚Äì with equal number of queries ‚Äì and we also find that using larger data (i.e., full MS MARCO) improves results. Our final models are trained on the full MS MARCO ‚Äì further ablations are conducted on the NQ data, for efficiency reasons.

Labeling strategies. As described in Section[3](https://arxiv.org/html/2501.16214v1#S3 "3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), we can train the pruner either to perform token-level labeling (with sentence rounding at inference) or to perform sentence-level labeling. In the former case sentence representations are richer but the model also needs to learn to output similar predictions for tokens inside one sentence. In the latter case sentence content must be represented in a single embedding which may limit representation expressivity. In practice we observe close performance, with the token-level strategy slightly outperforming the sentence-level one some datasets. In all other experiments we use the token-level strategy.

Oracle prompts. We compare three options for prompting an oracle LLM to generate silver labeling: (1)answer oracle: asking to answer the given question from the given context, citing corresponding sentences; (2)relevance oracle: asking to list any relevant information in the context to the question, citing corresponding sentences; (3)straightforward oracle: asking to output indexes of sentences which answer the given question. We found that the behavior of the straightforward oracle varies on different prompts, while the use of the answer oracle makes answers more consistent. The motivation for the relevance oracle is that often contexts contain distantly relevant information to the query and it could be reasonable to select the corresponding sentences. Comparing the listed prompts, we observe that the relevance oracle underperforms the answer oracle, and the straightforward oracle performs similarly or slightly lower than the answer oracle, which is used in all other experiments.

Unification with reranker. In Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") we compare `Provence` trained as a standalone model and as a model unified with reranker, and find that both strategies lead to similar results ‚Äì although the former relies on two separate inference steps (re-ranking and pruning) in a RAG pipeline.

5 Conclusion
------------

In this work, we present `Provence`, a robust, adaptable, and efficient context pruner for Question Answering ‚Äì either unified in a single model with reranking capabilities or available as a lightweight standalone model. In contrast to previous extractive approaches, `Provence` dynamically detects the needed pruning ratio for a given context and can be used out-of-the-box for various QA domains. In extensive experiments, we demonstrate that `Provence` prunes contexts with negligible to no drops in performance and in some cases even brings performance improvement due to removing context noise. We also show `Provence` capabilities in correctly detecting the number of relevant sentences in contexts, located at any position, and with contexts of various lengths. Finally, the ablation study highlights the importance of using a large training data and the appropriate prompt in the silver oracle.

