Title: Provence: efficient and robust context pruning for retrieval-augmented generation

URL Source: http://arxiv.org/html/2501.16214v1

Published Time: Tue, 28 Jan 2025 02:42:47 GMT

Markdown Content:
\correspondingauthor

nadia.chirkova@naverlabs.com, thibault.formal@naverlabs.com \affiliations NAVER LABS Europe, Grenoble, France \website https://huggingface.co/naver/provence-reranker-debertav3-v1 \websiteref[](https://huggingface.co/naver/provence-reranker-debertav3-v1)\teaserfig![Image 1: [Uncaptioned image]](https://arxiv.org/html/2501.16214v1/extracted/6159612/Provence_ill.png)

###### Abstract

Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are however limited, and do not provide a universal model that would be both efficient and robust in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work.

1 Introduction
--------------

Retrieval-Augmented Generation (RAG) has become a widely-used paradigm for improving factuality, attribution, and adaptability of Large Language Models (LLMs) (Das et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib13); Asai et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib3); Seo et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib46); Lewis et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib30); Mallen et al., [2023a](https://arxiv.org/html/2501.16214v1#bib.bib35); Min et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib37)). Augmenting a given user‚Äôs query with retrieved relevant contexts helps to avoid the generation of untruthful information and enables the provision of references used to generate the answer. Furthermore, using a domain-specific datastore may enable access and reasoning over a previously unknown knowledge ‚Äì without fine-tuning the LLM. One additional advantage of the RAG approach is the easy plug-and-play architecture ([LangChain,](https://arxiv.org/html/2501.16214v1#bib.bib27)): practitioners may choose components (retrievers, generator LLMs, context granularity etc.) which best suit their particular cases to maximize the final performance.

![Image 2: Refer to caption](https://arxiv.org/html/2501.16214v1/x1.png)

Figure 1: Illustration of inference (left) and training (right) of Provence.

Table 1: Analysis of existing approaches for context pruning. Violet / Orange highlight practical / less-practical solutions. 

At the same time, the use of RAG adds computational overhead due to both retrieval latency and the increased input length for the LLMs. It may also propagate irrelevant information present in retrieved contexts into generated responses. These issues can be solved by developing more efficient and robust LLMs ‚Äì either by making architectural changes to process long contexts more efficiently (Nawrot et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib38); Dao, [2024](https://arxiv.org/html/2501.16214v1#bib.bib12); Chevalier et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib7); Louis et al., [2025](https://arxiv.org/html/2501.16214v1#bib.bib34)) or increasing the diversity of the tuning data to improve processing of irrelevant contexts (Lin et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib33)). However, tuning the LLM can be highly resource-consuming, or even impossible to apply for proprietary (closed) LLMs. An alternative solution consists in pruning retrieved contexts by removing context parts irrelevant to the user‚Äôs query ‚Äì which reduces context lengths and therefore speeds up generation. Such context pruning module can be used in a plug-and-play manner with any generator LLM, featuring both easy use and better transparency in the RAG pipeline.

Despite initial efforts on developing context pruners for RAG, none of the existing solutions provide a model ready to be used out-of-the-box in practice. First, many approaches are designed for a simplified setting, e.g., with the assumption that only one sentence per context is relevant to the input query (Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50); Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)), or that the compression ratio is fixed (Jiang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib22); Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)). However, in practice contexts may contain various portions of relevant information, from empty to full relevant context, and pruners should detect it in an adaptable fashion. Second, many works introduce context pruners that are not efficient enough to be used in practice. This includes using billion-sized LLMs as base models for pruners (Jiang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib23); Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42); Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50)), or designing abstractive context compressors which require sequential autoregressive generation of the final context (Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50); Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)). We argue that a more practical and efficient setting consists in fine-tuning a small-size model such as DeBERTa(He et al., [2021b](https://arxiv.org/html/2501.16214v1#bib.bib17), [a](https://arxiv.org/html/2501.16214v1#bib.bib16)), as an extractive pruner, i.e., with a lightweight prediction head for selecting relevant context parts. Third, most of the existing works train context pruners for each dataset individually and do not target nor test pruners robustness to various data domains.

Table[1](https://arxiv.org/html/2501.16214v1#S1.T1 "Table 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") summarizes the properties of various existing methods along specified dimensions and shows that none of them satisfy all listed criteria. The table also includes a dimension of pruning granularity, i.e., token-level vs sentence-level pruning. In this work, we focus on query-dependent sentence-level pruning, which prunes out semantic units (sentences) that are deemed not relevant to generate the answer. An alternative approach is token-level pruning which prunes out low-level grammatical units such as articles or interjections, usually in a query-independent fashion. The two approaches are orthogonal and could potentially be combined.

To address listed limitations, we introduce `Provence` (Pruning and Reranking Of retrieVEd relevaNt ContExt), an approach for training an adaptable, efficient and robust sentence-level context pruner for Question Answering, which can be used out-of-the-box across various domains and settings. To achieve this, we formulate context pruning as binary sequence labeling so that the binary mask predicted by the pruner determines sentences (from zero to all) which are relevant to the query, and train our pruner from a lightweight DeBERTa model on diverse data. Furthermore, we notice that context pruning and reranking (i.e., the second step in effective retrieval pipelines) bear a strong resemblance. We therefore propose to unify these two models into a single one, completely eliminating the cost of context pruning in the RAG pipeline.

More specifically, our contributions are as follows:

*   ‚Ä¢We propose an approach for training an adaptable, robust, and efficient context pruner for QA ‚Äì and release our trained model. Three key ingredients of our approach are formulating context pruning as sequence labeling, unifying context pruning and reranking in a single model, and training on diverse data. 
*   ‚Ä¢We test `Provence` on various QA domains and show its out-of-the-box applicability to prune contexts with negligible to no drop in performance and at almost no cost, substantially outperforming baseline approaches. We also demonstrate `Provence` capabilities in detecting the number of relevant sentences at any positions in the context and robustness to various context lengths. 
*   ‚Ä¢We conduct multiple ablations to demonstrate which techniques are essential for training robust context pruners, to provide insights for future context pruners development. 

#### Definitions.

A typical RAG pipeline consists of (0) a user‚Äôs question, or query; (1) a datastore, i.e., a collection of documents (pieces of text) to be retrieved from, (2) an efficient retriever which enables fast retrieval from a large datastore (typically a dual-encoder model, where queries and passages are encoded independently), (3) a more expensive cross-encoder reranker which further reduces and reorders a set of retrieved passages (cross-encoding means encoding a passage together with a query); and (4) a generator LLM which outputs the final response based on the user‚Äôs query and the relevant passages. Such a pipeline can be represented as retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>generate. Context pruning can be incorporated before generation, i.e., retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>prune>>much-greater-than>>>>generate. In our work, we also propose to incorporate context pruning into reranking, an essential and already present component in RAG(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)): retrieve>>much-greater-than>>>>rerank+prune>>much-greater-than>>>>generate. This enables context pruning at almost zero cost.

2 Related work
--------------

Context pruning. RECOMP(Xu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib52)) focuses on context pruning for RAG and proposes both extractive and abstractive context pruners. The extractive RECOMP approach independently encodes sentences in the context and then selects top sentences with embeddings closest to the query embedding. Such an approach limits context understanding, due to independent processing of both sentences and queries. The method also requires specifying the amount of sentences to keep as a hyperparameter ‚Äì which is usually unknown in practice and should depend on each particular passage. The abstractive RECOMP summarizes key information from the passage relevant to the query (including zero relevant information) by training on silver summaries generated by GPT-3.5. However, it requires inefficient autoregressive generation of the final context, and can eventually hallucinate facts not present in the input context. FilCo(Wang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib50)) similarly proposes to generate contexts autoregressively but is trained on extractive targets, i.e., one sentence from the context selected by one of three criteria. The drawbacks are again inefficiency and the simplified assumption of one relevant sentence per context. A recent approach, COMPACT(Yoon et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib54)), also proposes to generate filtered contexts autoregressively ‚Äì hence inefficiently ‚Äì and introduces an iterative approach for gradually updating the relevant context after processing a new portion of retrieved passages. In contrast to all listed efforts, `Provence` dynamically detects the amount of relevant information in the context ‚Äì from zero to all sentences ‚Äì in an extractive and efficient way. Furthermore, we propose a novel approach of integrating context pruning into a reranker.

Concurrently to our work, DSLR (Hwang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib20)) performs extractive sentence-level pruning, by encoding sentences one-by-one, together with the query, using existing rerankers. Similarly to Provence, DSLR keeps sentences with scores higher than a threshold and preserves the original order of sentences. However, in contrast to Provence, DSLR is not capable of keeping groups of semantically connected sentences, due to independent sentence processing.

An orthogonal line of work proposes extractive token-level pruners. LLMLingua(Jiang et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib22)) and Selective Context (Li et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib31)) use LLMs to remove tokens with high generation probabilities, independently of the query. LLMLingua2(Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)) is a small BERT-based model finetuned to eliminate redundant tokens, also independently of the query. LongLLMLingua(Jiang et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib23)) proposes query-dependent LLM-based token pruning based on contrastive perplexity. Listed approaches remove tokens in a way that it does not break context understanding for the LLM ‚Äì hence they are not capable of removing semantic parts of the context. LLMLingua models also have many hyperparameters in the interface which are hard to tune in practice. These approaches can however also be combined with sentence-level pruning.

Retrieval granularity. Alternatively to context pruning, one can reformulate datastore content into atomic units, e.g., propositions as in Dense-X retrieval Chen et al. ([2024c](https://arxiv.org/html/2501.16214v1#bib.bib6)) or decontextualized sentences(Choi et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib8)). Such preprocessing is expensive and can lead to some information loss.

Passage filtering. Another related ‚Äì and orthogonal ‚Äì line of works focuses on filtering entire passages if they are deemed irrelevant for a given question; such an approach can be straightforwardly combined with `Provence`. A simple method consists in introducing a threshold on the (re)ranking score. LongLLMLingua reranks passages based on the probability of a question given the passage. (Yoran et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib55)) use natural language inference models to filter out passages that do not entail question-answer pairs, but report that this approach sometimes filters out relevant passages too.

Improving context processing in LLMs. While context pruners aim to remove context parts irrelevant to the user‚Äôs query, another line of work aims to process contexts more efficiently and effectively in LLMs. Efficient context processing could be achieved through efficient attention implementations(Dao, [2024](https://arxiv.org/html/2501.16214v1#bib.bib12); Anagnostidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib2)), KV cache compression(Nawrot et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib38)), encoding retrieved passages in parallel(Zhu et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib56)), or compressing contexts into one or more context embeddings(Chevalier et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib7); Ge et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib15); Rau et al., [2024b](https://arxiv.org/html/2501.16214v1#bib.bib45); Louis et al., [2025](https://arxiv.org/html/2501.16214v1#bib.bib34)). Other works aim to make LLMs more robust, by exposing them to noisy contexts during training or finetuning(Izacard et al., [2022](https://arxiv.org/html/2501.16214v1#bib.bib21); Lin et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib33)). All such approaches usually require LLM adaptation which may complicate application to an arbitrary picked LLM.

3 Provence
----------

The high-level overview of our proposed approach is illustrated in Figure[1](https://arxiv.org/html/2501.16214v1#S1.F1 "Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). Our first contribution is to pose the context pruning problem as a sequence labeling task. We fine-tune a DeBERTa model to encode the query‚Äìcontext pair and output binary masks which are used to filter out irrelevant context parts. The labels for training are generated by LLama-3-8B-Instruct(AI@Meta, [2024](https://arxiv.org/html/2501.16214v1#bib.bib1)); we call them silver labels since they are generated automatically. Such an approach solves several limitations of existing context pruners: (1) by construction, the model is able to deal with varying noise in contexts and select an appropriate pruning ratio; (2) queries are encoded together with context sentences (cross-encoding), providing richer representations ‚Äì compared for instance to extractive RECOMP which encodes query and context sentences independently; (3) using a lightweight encoder makes our approach more efficient than LLM-based or abstractive methods.

Our second contribution consists in unifying reranking and context pruning ‚Äì instead of considering these steps as distinct in the RAG pipeline. In `Provence`, reranking and pruning can be done in a single forward step, thus eliminating the computational overhead due to context pruning ‚Äì making `Provence` almost ‚Äúfree‚Äù.

Training data. Our approach requires a set of training questions and a retrieval datastore. Speficially, we rely on the train set of the MS MARCO document ranking collection which includes 370 k ùëò k italic_k queries(Nguyen et al., [2016](https://arxiv.org/html/2501.16214v1#bib.bib40)). The MS MARCO collection is a domain-diverse datastore of 3.2‚Å¢M 3.2 ùëÄ 3.2M 3.2 italic_M documents crawled from the Web ‚Äì which is required for the final model‚Äôs robustness to various domains ‚Äì and is often used to train retrievers and rerankers. We also consider the train set of Natural Questions which contains 87 k ùëò k italic_k queries Kwiatkowski et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib25)).

Data processing. We create a retrieval datastore by splitting MS MARCO documents into passages consisting of N ùëÅ N italic_N consecutive sentences ‚Äì N ùëÅ N italic_N being a random integer ‚àà1..10 absent 1..10\in 1..10‚àà 1..10. This is to enable the pruner‚Äôs robustness to variable retrieved context lengths. We also prepend page titles to each passage. For each question, we retrieve top-5 relevant passages using a strong retrieval pipeline(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)) consisting of a SPLADE-v3 retriever(Lassance et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib29)) and a DeBERTa-v3 reranker(Lassance & Clinchant, [2023](https://arxiv.org/html/2501.16214v1#bib.bib28)). The resulting set of retrieved passages is naturally diverse w.r.t. relevance or irrelevance to the question, due to imperfections in retrieval.

Silver labels generation. Given a question and a retrieved passage (context), we split the passage into sentences 1 1 1 using the nltk.sent_tokenize function: [https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html) and prompt Llama-3-8B-Instruct to select sentences relevant to the given question. One approach would be to use a straightforward prompt such as ‚ÄúOutput indexes of sentences relevant to the given question‚Äù. However, we decided to utilize the strong LLMs‚Äô capabilities of actually answering questions while citing relevant context sentences. We therefore instruct the LLM to answer the given question using only information provided in the given context, and output ‚ÄúNo answer‚Äù in case no relevant information is provided. We also specify the easy-to-parse citation format `[i]` and number sentences with the same marker in the context. Our prompt can be found in Appendix ‚Äì Table[6](https://arxiv.org/html/2501.16214v1#A2.T6 "Table 6 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"); we use greedy decoding and parse cited sentences using regular expressions. We also compare different prompting strategies in the ablation study.

We found that Llama-3-8B is well capable of answering only based on a given context in most cases and of outputting a citation ‚àº90%similar-to absent percent 90\sim 90\%‚àº 90 % of the time. We filter out cases when no citations are produced and "No answer" is not present in the LLM‚Äôs output, as these are the cases when the context actually contains relevant information but the LLM ‚Äúforgot‚Äù to cite it. The final labels distribution (number of selected sentences per context, their positions) is shown in Appendix ‚Äì Figure[5](https://arxiv.org/html/2501.16214v1#A2.F5 "Figure 5 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Training of Provence. Our context pruner receives as input the concatenation of a question and a retrieved context, and outputs per-token binary labels denoting whether each token (defined by the pretrained model‚Äôs tokenizer) should be included in the selected context. In Section[4.4](https://arxiv.org/html/2501.16214v1#S4.SS4 "4.4 Ablations ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Ablations), we also consider an approach where a special token is inserted at the beginning of each sentence, and labels are predicted per-sentence based on the representations of those tokens. We train `Provence` as a binary per-token classifier with ground truth labels coming from the silver data labeling, and the model can be used as a standalone pruner, i.e., retrieve>>much-greater-than>>>>rerank>>much-greater-than>>>>Provence (standalone)>>much-greater-than>>>>generate.

Unifying compression and reranking. We note that cross-encoder rerankers(Nogueira & Cho, [2020](https://arxiv.org/html/2501.16214v1#bib.bib41)) share both the same architecture and inputs (pairs of question‚Äìpassages) as `Provence`. Additionally, the task of context pruning (selecting parts of contexts that are useful for generating the answer to the question) intrinsically bears similarity with re-ranking (estimating the relevance of a context w.r.t. the question) ‚Äì and we hypothesize the possibility of knowledge transfer between these two related tasks. We therefore propose to unify both approaches in a single model, with two different task heads. More specifically, the reranking head outputs a scalar prediction for the `BOS` token while the pruning head outputs per-token predictions for the passage tokens, as illustrated in Figure[1](https://arxiv.org/html/2501.16214v1#S1.F1 "Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). To ease training, we propose to further fine-tune a pretrained reranker on our labeling objective, while adding a ranking ‚Äúregularizer‚Äù to preserve initial reranking capabilities. The regularizer is a Mean Squared Error loss on the reranking scores from the initial reranker. This can be viewed as a straightforward pointwise score distillation process, where the initial model serves as the teacher ‚Äì a method that has demonstrated great effectiveness in Information Retrieval Hofst√§tter et al. ([2021](https://arxiv.org/html/2501.16214v1#bib.bib18)). The final loss function is as follows:

‚Ñí=‚àën=1 N{‚àëk=1 L n log‚Å°P‚Å¢(y n,k|z n,k)+Œª‚Å¢(s n‚àíz n,0)2}z n=Provence‚Å¢(x n)‚Ñí superscript subscript ùëõ 1 ùëÅ superscript subscript ùëò 1 subscript ùêø ùëõ ùëÉ conditional subscript ùë¶ ùëõ ùëò subscript ùëß ùëõ ùëò ùúÜ superscript subscript ùë† ùëõ subscript ùëß ùëõ 0 2 subscript ùëß ùëõ Provence subscript ùë• ùëõ\begin{split}\mathcal{L}=\sum_{n=1}^{N}\biggl{\{}\sum_{k=1}^{L_{n}}\log P(y_{n% ,k}|z_{n,k})+\lambda\bigl{(}s_{n}-z_{n,0}\bigr{)}^{2}\biggr{\}}\\ z_{n}=\text{Provence}(x_{n})\end{split}start_ROW start_CELL caligraphic_L = ‚àë start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT { ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT roman_log italic_P ( italic_y start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT ) + italic_Œª ( italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT - italic_z start_POSTSUBSCRIPT italic_n , 0 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT } end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = Provence ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) end_CELL end_ROW(1)

where N ùëÅ N italic_N is the number of datapoints (query‚Äìpassage pairs), x n subscript ùë• ùëõ x_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n+1 subscript ùêø ùëõ 1 L_{n}+1 italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 input tokens (concatenated query, passage and `BOS` at the 0-t‚Å¢h ùë° ‚Ñé th italic_t italic_h position), z n subscript ùëß ùëõ z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n+1 subscript ùêø ùëõ 1 L_{n}+1 italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 predictions output by the model, y n subscript ùë¶ ùëõ y_{n}italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is a sequence of L n subscript ùêø ùëõ L_{n}italic_L start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT target binary labels for context pruning, s n subscript ùë† ùëõ s_{n}italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT is the teacher score (initial reranker), z n,0 subscript ùëß ùëõ 0 z_{n,0}italic_z start_POSTSUBSCRIPT italic_n , 0 end_POSTSUBSCRIPT is the ranking score predicted from the `BOS` representation.

In the case of the unified model, re-ranking and context pruning need a single forward step from the encoder, i.e., retrieve>>much-greater-than>>>>Provence (w/ re-ranking)>>much-greater-than>>>>generate ‚Äì making context pruning almost free in terms of execution time.

Inference with Provence. At inference, we feed a concatenation of a question and a retrieved passage through `Provence`, which outputs probabilities of including each token in the final context, as well as the passage score in the case of the unified model. We simply use a threshold T ùëá T italic_T to binarize the token probabilites (keep or not) ‚Äì which has a direct effect on the compression rate. As shown in the experiments Section, the choice of a threshold is generally transferable across various datasets, making the model flexible to be used out-of-the box in various QA applications 2 2 2 Note that tuning the threshold per dataset could of course further improve results..

We note that our model outputs token-level predictions despite the sentence-level labeling task. We found that probabilities of including tokens into the final context are naturally clustered on the sentence level ‚Äì see example in Appendix Figure[6](https://arxiv.org/html/2501.16214v1#A2.F6 "Figure 6 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") ‚Äì due to the sentence-level targets used in training. However, in rare cases we could still have partial sentences being selected. To avoid this phenomenon, we apply a ‚Äúsentence rounding‚Äù procedure: for each sentence, we check the ratio of kept tokens (predicted label=1 absent 1=1= 1), and select the entire sentence only if it is higher than 0.5 0.5 0.5 0.5.

![Image 3: Refer to caption](https://arxiv.org/html/2501.16214v1/x2.png)

Figure 2: Main results for various QA domains, comparing Provence and baseline models. Generator: LLama-2-7B, retriever: SPLADE-v3, reranker: DeBERTa-v3 (or Provence in the unified setting). Plot titles denote ‚ÄúDataset name (datastore type)‚Äù. x ùë• x italic_x-axis denotes QA performance evaluated with LLM-as-a-judge; y ùë¶ y italic_y-axis denotes the context compression ratio. For both metrics, the higher the better: the best model would be closest to the top right corner. Numerical scores are presented in App. Tables[8](https://arxiv.org/html/2501.16214v1#A2.T8 "Table 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")‚Äì[9](https://arxiv.org/html/2501.16214v1#A2.T9 "Table 9 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). Main conclusion: Provence consistently lies on the Pareto front.

4 Experiments
-------------

### 4.1 Experimental setup

Provence training details. We train `Provence` on the data described in Section[3](https://arxiv.org/html/2501.16214v1#S3 "3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), using PyTorch(Paszke et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib43)) and HuggingFace transformers(Wolf et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib51)). We use DeBERTa-v3(He et al., [2021a](https://arxiv.org/html/2501.16214v1#bib.bib16)) as our pretrained model for training the standalone `Provence`. For the unified approach, we start training from an already trained cross-encoder, also based on DeBERTa-v3(Lassance & Clinchant, [2023](https://arxiv.org/html/2501.16214v1#bib.bib28)). Note that in the latter, we initialize the ranking head from its fine-tuned version, and train the separate pruning head from scratch.

After preliminary experiments, we set the learning rate to 3√ó10‚àí6 3 superscript 10 6 3\times 10^{-6}3 √ó 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT, the batch size to 48 and train models for one epoch. For joint training, there is a slight trade-off between pruning and reranking. We set the reranking regularization coefficient Œª ùúÜ\lambda italic_Œª to 0.05 0.05 0.05 0.05, chosen as the minimal value that does not substantially degrade reranking performance on the MS MARCO development set.

Table 2: Time/MFLOPS required for context pruning, 50 samples, with batch size set 1 (1 sample consists of a query and top retrieved documents). Top-5 retrieved documents.

Table 3: Speed up in generation due to compression (Provence, 49% compression). Batch sizes 1 or 256. 

Evaluation datasets. We test `Provence` on a diverse set of QA datasets. First, we consider commonly used datasets relying on Wikipedia datastore: Natural Questions(Kwiatkowski et al., [2019](https://arxiv.org/html/2501.16214v1#bib.bib25)), TyDi QA(Clark et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib9)), PopQA(Mallen et al., [2023b](https://arxiv.org/html/2501.16214v1#bib.bib36)) (all single-hop questions), and HotpotQA(Yang et al., [2018](https://arxiv.org/html/2501.16214v1#bib.bib53)) (multi-hop questions). Second, we consider datasets with datastores from various domains: BioASQ(Nentidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib39)) (biomedical questions with Pubmed as a datastore), SyllabusQA(Fernandez et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib14)) (questions about educational course logistics, with courses syllabus as a datastore); and RGB Chen et al. ([2024b](https://arxiv.org/html/2501.16214v1#bib.bib5)) (questions about news with Google-searched news as contexts). Further details can be found in Appendix[A](https://arxiv.org/html/2501.16214v1#A1 "Appendix A Data ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Evaluation settings. We conduct experiments using BERGEN(Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)), a benchmarking library for RAG, using the recommended experimental setting. For each query, we retrieve top-5 relevant passages using a strong and robust retrieval pipeline: SPLADE-v3 >>much-greater-than>>>> DeBERTa-v3 reranker (except for RGB, for which Google-searched passages are already provided). We then pass queries prepended with relevant document (full length or pruned) into LLama-2-7B-chat(Touvron et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib49))3 3 3 For main experiments, we chose a ‚Äúweaker‚Äù generator which relies more on contexts, to create a more challenging setting for context pruners; results with stronger generators are reported in Appendix ‚Äì Figure[8](https://arxiv.org/html/2501.16214v1#A2.F8 "Figure 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). to generate answers; other retrieval-generator settings are further reported in Appendix. Each evaluation dataset comes with short keyword answers, which we use to evaluate responses using LLM-based evaluation (LLMeval in Rau et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib44)); match-based metrics are also reported in Appendix. We additionally measure compression as a portion of the context which was pruned out.

We compare `Provence` to publicly available context pruning models listed in Table[1](https://arxiv.org/html/2501.16214v1#S1.T1 "Table 1 ‚Ä£ 1 Introduction ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), except LLMLingua and Selective Context which were shown to underperform LLMLingua2(Pan et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib42)). For all context pruners (except abstractive RECOMP for which it is not available), we enforce the selection of the first (title) passage sentence, to avoid ambiguity in understanding the context by the generator. For extractive RECOMP, we use the model trained on NQ, consider using top-1/2/3 sentences, and prepend the passage title to each sentence. For the LLMLingua family, we vary the compression rate in {0.25,0.5,0.75}0.25 0.5 0.75\{0.25,0.5,0.75\}{ 0.25 , 0.5 , 0.75 } and use code provided on the official repository 4 4 4[https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua). We use the XLM-RoBERTa model for LLMLingua2. For `Provence`, we use T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1 and T=0.5 ùëá 0.5 T=0.5 italic_T = 0.5. We also compare our method to DSLR based on the same reranker as ours, i.e, DeBERTa-v3.

### 4.2 Main results

Context pruners are often only tested on limited domain data, e.g., with Wikipedia datastore, and an important aspect of our work is evaluating context pruning on a series of QA domains. Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports the trade-off between compression (efficiency) and LLM-evaluated performance (quality), for various QA datasets and context pruning methods. We choose to report a figure per dataset to better assess the Pareto front of existing solutions, rather than comparing methods with different compression rates in the same table. Figure[7](https://arxiv.org/html/2501.16214v1#A2.F7 "Figure 7 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") in Appendix further reports similar results with match-based metric, and Appendix Tables[11](https://arxiv.org/html/2501.16214v1#A2.T11 "Table 11 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")‚Äì[13](https://arxiv.org/html/2501.16214v1#A2.T13 "Table 13 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") show examples of context pruning with various methods.

First, we observe that `Provence` achieves the highest performance across pruning methods, for similar compression ratios. Second, it is noteworthy that `Provence` outperforms methods requiring more computations such as LLMLingua models, showing that efficiency is not traded for effectiveness. Furthermore, `Provence` is the only method capable of achieving high compression levels without (or with negligible) performance drops, on all datasets. Moreover, for some datasets, e.g., PopQA, pruning with `Provence` leads to performance improvements due to noise filtering.

![Image 4: Refer to caption](https://arxiv.org/html/2501.16214v1/x3.png)

![Image 5: Refer to caption](https://arxiv.org/html/2501.16214v1/x4.png)

![Image 6: Refer to caption](https://arxiv.org/html/2501.16214v1/x5.png)

Figure 3: Analyses. (Left) Needle-in-the-haystack test allowing the control of the position of the ground truth sentence(s) in the context. (Middle) Comparison of the number of selected sentences by the silver predictor (LLaMA-3-8B-Instruct) and Provence. Heatmaps are normalized by rows: a cell in position (i,j)ùëñ ùëó(i,j)( italic_i , italic_j ) indicates which percentage of contexts that were pruned into i ùëñ i italic_i sentences by the silver predictor, were pruned into j ùëó j italic_j sentences by Provence. (Right) Testing Provence in settings with different context lengths. All experiments are done with unified Provence, T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1.

The effect of threshold. An important aspect in the out-of-the-box applicability of context pruners is how much effort is needed to select the suitable values of hyperparameters. For `Provence`, it only consists in setting the pruning threshold T ùëá T italic_T. In Figure [2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (for which T=0.1 ùëá 0.1 T=0.1 italic_T = 0.1 and T=0.5 ùëá 0.5 T=0.5 italic_T = 0.5), we observe that `Provence` pruning ratio automatically varies from 50% to 80%, depending on the dataset, which demonstrates that the same values for T ùëá T italic_T work well for all considered domains ‚Äì making Provence robust to the choice of hyperparameters. If necessary, users can still tune it further for their datasets and/or needs. We note that some models specify the desired compression ratio as a hyperparameter, e.g., LLMLingua models or extractive RECOMP (through top-N ùëÅ N italic_N sentences). While it may seem convenient to estimate inference cost, the ‚Äúoptimal‚Äù compression ratio (without losing performance) is specific to each particular question-context pair. Thus, using a threshold as a hyperparameter is more appropriate for this task. We also experimented with specifying a threshold in extractive RECOMP (shown on the same plot) and found that it often leads to lower performance (compared to top-N ùëÅ N italic_N). The reason is that different queries have different ranges of similarity scores.

Efficiency. We compare `Provence` with other pruning methods in terms of efficiency. Table [2](https://arxiv.org/html/2501.16214v1#S4.T2 "Table 2 ‚Ä£ 4.1 Experimental setup ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports compression time and MFLOPS 5 5 5 We use the PyTorch profiler to report FLOPS required by each pruner. required by different pruning methods. As expected, LongLLMLingua (based on LLama-2-7B-chat) is the slowest context pruner. RECOMP abstr. requires less MFLOPS compared to `Provence`, but its autoregressive nature makes it slower in practice 6 6 6 This highlights the fact that MFLOPS do not always align with real inference time, due to different architectural choices.. Note that in the case of the unified model, pruning is almost free ‚Äì as it‚Äôs part of the re-ranking step. Table [3](https://arxiv.org/html/2501.16214v1#S4.T3 "Table 3 ‚Ä£ 4.1 Experimental setup ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") reports speed-up gains due to compression with `Provence` model (‚àºsimilar-to\sim‚àº 50% compression rate). All runs were performed on single Tesla V100-SXM2-32GB GPU with vllm Kwon et al. ([2023](https://arxiv.org/html/2501.16214v1#bib.bib26)). With large batch sizes, we systematically observe 2√ó\times√ó speed-ups at inference, while smaller batch sizes lead to lower gains (especially for smaller models). We assume this is mostly due to the CPU/GPU communication bottleneck, which masks inference gains due to compression.

### 4.3 Analysis

In this Section, we conduct a more fine-grained evaluation to better understand the properties of `Provence`.

Robustness to the position of relevant information in the context. We design a needle-in-the-haystack experiment which allows us to check the performance of `Provence` on a simple toy example and to evaluate its robustness w.r.t. the position of the relevant information in the input context. We write 5 questions and answers 7 7 7 Example: ‚ÄúWhich library was used in the experiments?‚Äù, answer: ‚ÄúExperiments were conducted using the Bergen library‚Äù. Example reformulation into a 2-sentence answer: ‚ÄúExperiments were conducted using a library. Its name is Bergen.‚Äù, and insert answers (‚Äúneedles‚Äù) at random positions between sentences, in a subset of 100 passages sampled from the Wikipedia datastore. Ideally, `Provence` should only select the ‚Äúneedle‚Äù sentences and filter out all other sentences in contexts. We plot the number of selected sentences and percentage of cases when the pruned context contains the ‚Äúneedle‚Äù (Figure [3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), (Left)). We consider two settings: with 1- and 2-sentence ‚Äúneedles‚Äù. We observe that `Provence` correctly selects ‚Äúneedle‚Äù sentence(s) in most cases, except at leftmost and rightmost positions.8 8 8 The reason for the drops in the left-most and right-most positions is that training data has little examples of the corresponding types of relevant sentences, see e.g., statistics for the rightmost position in the App. Figure 5, (Right). We plan to work on further improving processing of these positions in future work. In most cases `Provence` does not select any irrelevant sentences. The results are similar for both simpler (1-sentence) and harder (2-sentence) ‚Äúneedles‚Äù showing `Provence`‚Äôs flexibility in detecting the number of relevant sentences, discussed below in more details.

Adaptability to the variable number of relevant sentences. To evaluate the capability of `Provence` to dynamically detect the number of relevant sentences in the context, we compare the number of sentences L ùêø L italic_L selected by `Provence` and by a silver oracle, for question-context examples from various datasets. A silver oracle is easy to construct for L=0 ùêø 0 L=0 italic_L = 0, by pairing questions with randomly sampled contexts. For L‚©æ1 ùêø 1 L\geqslant 1 italic_L ‚©æ 1, we use the labeling produced by Llama-3-8B-Instruct. Figure[3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Middle) shows that the number of relevant sentences detected by `Provence` is close to the silver oracle value in most cases, for all considered datasets. In contrast, extractive RECOMP would always select a prespecified number of sentences.

![Image 7: Refer to caption](https://arxiv.org/html/2501.16214v1/x6.png)

Figure 4: Ablation results. All models are single-component modifications of the anchor model, which is a base-size model, trained on NQ data, with the answer oracle and token-level labeling. Numeric scores for this figure are duplicated in Appendix Table[10](https://arxiv.org/html/2501.16214v1#A2.T10 "Table 10 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), and results with match-based metrics are presented in Appendix ‚Äì Figure[11](https://arxiv.org/html/2501.16214v1#A2.F11 "Figure 11 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Robustness w.r.t. context granularity. Figure [3](https://arxiv.org/html/2501.16214v1#S4.F3 "Figure 3 ‚Ä£ 4.2 Main results ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Right) shows `Provence` performance for two datasets, with Wikipedia datastores made of contexts of various granularity. Here, each considered datastore is produced by splitting Wikipedia pages into chunks of N ùëÅ N italic_N sentences, N‚àà{2,6,10}ùëÅ 2 6 10 N\in\{2,6,10\}italic_N ‚àà { 2 , 6 , 10 }, or 100 words, and prepending the page title to each chunk. `Provence` shows high performance in all cases ‚Äì the performance with pruned contexts being close to the performance obtained using original contexts. As could be expected, the compression ratio is higher for longer contexts.

Reranking effectiveness. Table[4](https://arxiv.org/html/2501.16214v1#S4.T4 "Table 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") compares reranking performance between our reranking baseline and unified `Provence` ‚Äì whose training starts from the former. We can see that our joint training procedure (on both pruning and ranking tasks) makes it possible to learn a context pruner that preserves initial reranking capabilities. We further include as a comparison point results from a model trained in similar conditions on NQ. Overall, results are similar ‚Äì further highlighting the robustness of Provence w.r.t. training data. We further discuss such aspects in Section[4.4](https://arxiv.org/html/2501.16214v1#S4.SS4 "4.4 Ablations ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (Ablations).

Applicability in different settings. Figure[8](https://arxiv.org/html/2501.16214v1#A2.F8 "Figure 8 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") (App.) demonstrates the applicability of `Provence` in variable retrieval-generator settings ‚Äì achieving similar results as the ones reported in Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Table 4: Effectiveness of reranking top-50 documents retrieved by SPLADE-v3. DeBERTa-v3 is the ‚Äúbaseline‚Äù (initialization point for Provence, which we aim to preserve performance). We report the R@5 on two RAG datasets (NQ and HotpotQA), MRR@10 on MS MARCO passages (dev set), nDCG@10 on TREC DL‚Äô19(Craswell et al., [2020](https://arxiv.org/html/2501.16214v1#bib.bib10)), and mean nDCG@10 on the 13 open datasets from the BEIR benchmark(Thakur et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib48)) ‚Äì Table[7](https://arxiv.org/html/2501.16214v1#A2.T7 "Table 7 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") in Appendix reports the full results.

### 4.4 Ablations

In this Section we analyze various design choices made in `Provence` development, to provide insights into training context pruners for future works (results shown in Figure[4](https://arxiv.org/html/2501.16214v1#S4.F4 "Figure 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation")). All models in this section are standalone context pruners, trained with the same amount of parameter updates.

#### Model size.

We first observe that DeBERTa-large slighly increases the compression rate ‚Äì when comprared to DeBERTa-base. All other ablations are tuned from a DeBERTa-base model, for efficiency reasons. Note that the final `Provence` is trained from a DeBERTa-large model (or its equivalent reranker).

Data mixtures. We compare training on NQ (87 k ùëò k italic_k queries), MS MARCO downsampled to the same size, and full MS MARCO (370 k ùëò k italic_k queries). We observe that using the MS MARCO type of data performs similarly to training on NQ ‚Äì with equal number of queries ‚Äì and we also find that using larger data (i.e., full MS MARCO) improves results. Our final models are trained on the full MS MARCO ‚Äì further ablations are conducted on the NQ data, for efficiency reasons.

Labeling strategies. As described in Section[3](https://arxiv.org/html/2501.16214v1#S3 "3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"), we can train the pruner either to perform token-level labeling (with sentence rounding at inference) or to perform sentence-level labeling. In the former case sentence representations are richer but the model also needs to learn to output similar predictions for tokens inside one sentence. In the latter case sentence content must be represented in a single embedding which may limit representation expressivity. In practice we observe close performance, with the token-level strategy slightly outperforming the sentence-level one some datasets. In all other experiments we use the token-level strategy.

Oracle prompts. We compare three options for prompting an oracle LLM to generate silver labeling: (1)answer oracle: asking to answer the given question from the given context, citing corresponding sentences; (2)relevance oracle: asking to list any relevant information in the context to the question, citing corresponding sentences; (3)straightforward oracle: asking to output indexes of sentences which answer the given question. We found that the behavior of the straightforward oracle varies on different prompts, while the use of the answer oracle makes answers more consistent. The motivation for the relevance oracle is that often contexts contain distantly relevant information to the query and it could be reasonable to select the corresponding sentences. Comparing the listed prompts, we observe that the relevance oracle underperforms the answer oracle, and the straightforward oracle performs similarly or slightly lower than the answer oracle, which is used in all other experiments.

Unification with reranker. In Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") we compare `Provence` trained as a standalone model and as a model unified with reranker, and find that both strategies lead to similar results ‚Äì although the former relies on two separate inference steps (re-ranking and pruning) in a RAG pipeline.

5 Conclusion
------------

In this work, we present `Provence`, a robust, adaptable, and efficient context pruner for Question Answering ‚Äì either unified in a single model with reranking capabilities or available as a lightweight standalone model. In contrast to previous extractive approaches, `Provence` dynamically detects the needed pruning ratio for a given context and can be used out-of-the-box for various QA domains. In extensive experiments, we demonstrate that `Provence` prunes contexts with negligible to no drops in performance and in some cases even brings performance improvement due to removing context noise. We also show `Provence` capabilities in correctly detecting the number of relevant sentences in contexts, located at any position, and with contexts of various lengths. Finally, the ablation study highlights the importance of using a large training data and the appropriate prompt in the silver oracle.

#### Limitations.

Despite `Provence` being ready to use in various settings, demonstrated in the paper, it is focusing only on QA applications, with a single passage processed at a time, and is trained on English-only data. Future work could consider extending it to other tasks, multi-passage contexts, and languages beyond English.

References
----------

*   AI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md). 
*   Anagnostidis et al. (2023) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur√©lien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. _ArXiv_, abs/2305.15805, 2023. URL [https://api.semanticscholar.org/CorpusID:258888224](https://api.semanticscholar.org/CorpusID:258888224). 
*   Asai et al. (2024) Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. Reliable, adaptable, and attributable language models with retrieval. _arXiv preprint arXiv:2403.03187_, 2024. 
*   Chen et al. (2024a) Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024a. URL [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216). 
*   Chen et al. (2024b) Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(16):17754‚Äì17762, Mar. 2024b. [10.1609/aaai.v38i16.29728](https://arxiv.org/doi.org/10.1609/aaai.v38i16.29728). URL [https://ojs.aaai.org/index.php/AAAI/article/view/29728](https://ojs.aaai.org/index.php/AAAI/article/view/29728). 
*   Chen et al. (2024c) Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense X retrieval: What retrieval granularity should we use? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_, pp. 15159‚Äì15177, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. URL [https://aclanthology.org/2024.emnlp-main.845](https://aclanthology.org/2024.emnlp-main.845). 
*   Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 3829‚Äì3846, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.232](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.232). URL [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232). 
*   Choi et al. (2021) Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. Decontextualization: Making sentences stand-alone. _Transactions of the Association for Computational Linguistics_, 9:447‚Äì461, 2021. [10.1162/tacl_a_00377](https://arxiv.org/doi.org/10.1162/tacl_a_00377). URL [https://aclanthology.org/2021.tacl-1.27](https://aclanthology.org/2021.tacl-1.27). 
*   Clark et al. (2020) Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. _Transactions of the Association for Computational Linguistics_, 8:454‚Äì470, 2020. [10.1162/tacl_a_00317](https://arxiv.org/doi.org/10.1162/tacl_a_00317). URL [https://aclanthology.org/2020.tacl-1.30](https://aclanthology.org/2020.tacl-1.30). 
*   Craswell et al. (2020) Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the trec 2019 deep learning track, 2020. URL [https://arxiv.org/abs/2003.07820](https://arxiv.org/abs/2003.07820). 
*   Craswell et al. (2021) Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. Ms marco: Benchmarking ranking models in the large-data regime. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR ‚Äô21, pp. 1566‚Äì1576, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380379. [10.1145/3404835.3462804](https://arxiv.org/doi.org/10.1145/3404835.3462804). URL [https://doi.org/10.1145/3404835.3462804](https://doi.org/10.1145/3404835.3462804). 
*   Dao (2024) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=mZn2Xyh9Ec](https://openreview.net/forum?id=mZn2Xyh9Ec). 
*   Das et al. (2019) Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. Multi-step retriever-reader interaction for scalable open-domain question answering. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=HkfPSh05K7](https://openreview.net/forum?id=HkfPSh05K7). 
*   Fernandez et al. (2024) Nigel Fernandez, Alexander Scarlatos, and Andrew Lan. SyllabusQA: A course logistics question answering dataset. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 10344‚Äì10369, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [10.18653/v1/2024.acl-long.557](https://arxiv.org/doi.org/10.18653/v1/2024.acl-long.557). URL [https://aclanthology.org/2024.acl-long.557](https://aclanthology.org/2024.acl-long.557). 
*   Ge et al. (2024) Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=uREj4ZuGJE](https://openreview.net/forum?id=uREj4ZuGJE). 
*   He et al. (2021a) Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing, 2021a. 
*   He et al. (2021b) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentangled} {attention}. In _International Conference on Learning Representations_, 2021b. URL [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD). 
*   Hofst√§tter et al. (2021) Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, and Allan Hanbury. Improving efficient neural ranking models with cross-architecture knowledge distillation, 2021. URL [https://arxiv.org/abs/2010.02666](https://arxiv.org/abs/2010.02666). 
*   Hsia et al. (2024) Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Graham Neubig. Ragged: Towards informed design of retrieval augmented generation systems. _arXiv preprint arXiv:2403.09040_, 2024. 
*   Hwang et al. (2024) Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, and Jong Park. DSLR: Document refinement with sentence-level re-ranking and reconstruction to enhance retrieval-augmented generation. In Wenhao Yu, Weijia Shi, Michihiro Yasunaga, Meng Jiang, Chenguang Zhu, Hannaneh Hajishirzi, Luke Zettlemoyer, and Zhihan Zhang (eds.), _Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP_, pp. 73‚Äì92, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [10.18653/v1/2024.knowledgenlp-1.6](https://arxiv.org/doi.org/10.18653/v1/2024.knowledgenlp-1.6). URL [https://aclanthology.org/2024.knowledgenlp-1.6](https://aclanthology.org/2024.knowledgenlp-1.6). 
*   Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot Learning with Retrieval Augmented Language Models, November 2022. URL [http://arxiv.org/abs/2208.03299](http://arxiv.org/abs/2208.03299). arXiv:2208.03299 [cs]. 
*   Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 13358‚Äì13376, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.825](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.825). URL [https://aclanthology.org/2023.emnlp-main.825](https://aclanthology.org/2023.emnlp-main.825). 
*   Jiang et al. (2024) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1658‚Äì1677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL [https://aclanthology.org/2024.acl-long.91](https://aclanthology.org/2024.acl-long.91). 
*   Kim et al. (2023) Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling, 2023. 
*   Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453‚Äì466, 2019. 
*   Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. 
*   (27) LangChain. LangChain Documentation. [https://python.langchain.com/](https://python.langchain.com/). 
*   Lassance & Clinchant (2023) Carlos Lassance and St√©phane Clinchant. Naver labs europe (splade) @ trec deep learning 2022, 2023. URL [https://arxiv.org/abs/2302.12574](https://arxiv.org/abs/2302.12574). 
*   Lassance et al. (2024) Carlos Lassance, Herv√© D√©jean, Thibault Formal, and St√©phane Clinchant. Splade-v3: New baselines for splade, 2024. 
*   Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In _Advances in Neural Information Processing Systems_, volume 33, pp. 9459‚Äì9474. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html). 
*   Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 6342‚Äì6353, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.391](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.391). URL [https://aclanthology.org/2023.emnlp-main.391](https://aclanthology.org/2023.emnlp-main.391). 
*   Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR ‚Äô21, pp. 2356‚Äì2362, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380379. [10.1145/3404835.3463238](https://arxiv.org/doi.org/10.1145/3404835.3463238). URL [https://doi.org/10.1145/3404835.3463238](https://doi.org/10.1145/3404835.3463238). 
*   Lin et al. (2024) Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-augmented dual instruction tuning. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=22OTbutug9](https://openreview.net/forum?id=22OTbutug9). 
*   Louis et al. (2025) Maxime Louis, Herv√© D√©jean, and St√©phane Clinchant. Pisco: Pretty simple compression for retrieval-augmented generation. _arXiv preprint_, 2025. 
*   Mallen et al. (2023a) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9802‚Äì9822, Toronto, Canada, July 2023a. Association for Computational Linguistics. [10.18653/v1/2023.acl-long.546](https://arxiv.org/doi.org/10.18653/v1/2023.acl-long.546). URL [https://aclanthology.org/2023.acl-long.546](https://aclanthology.org/2023.acl-long.546). 
*   Mallen et al. (2023b) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9802‚Äì9822, Toronto, Canada, July 2023b. Association for Computational Linguistics. [10.18653/v1/2023.acl-long.546](https://arxiv.org/doi.org/10.18653/v1/2023.acl-long.546). URL [https://aclanthology.org/2023.acl-long.546](https://aclanthology.org/2023.acl-long.546). 
*   Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation, 2023. URL [https://arxiv.org/abs/2305.14251](https://arxiv.org/abs/2305.14251). 
*   Nawrot et al. (2024) Piotr Nawrot, Adrian La‚Äôncucki, Marcin Chochowski, David Tarjan, and E.Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference. _ArXiv_, abs/2403.09636, 2024. URL [https://api.semanticscholar.org/CorpusID:268384862](https://api.semanticscholar.org/CorpusID:268384862). 
*   Nentidis et al. (2023) Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima-L√≥pez, Eulalia Farr√©-Maduell, Luis Gasco Sanchez, Martin Krallinger, and Georgios Paliouras. _Overview of BioASQ 2023: The Eleventh BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering_, pp. 227‚Äì250. 09 2023. ISBN 978-3-031-42447-2. [10.1007/978-3-031-42448-9_19](https://arxiv.org/doi.org/10.1007/978-3-031-42448-9_19). 
*   Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. November 2016. URL [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/). 
*   Nogueira & Cho (2020) Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert, 2020. 
*   Pan et al. (2024) Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H.Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Findings of the Association for Computational Linguistics ACL 2024_, pp. 963‚Äì981, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-acl.57](https://aclanthology.org/2024.findings-acl.57). 
*   Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K√∂pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. _PyTorch: an imperative style, high-performance deep learning library_. Curran Associates Inc., Red Hook, NY, USA, 2019. 
*   Rau et al. (2024a) David Rau, Herv√© D√©jean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, St√©phane Clinchant, and Vassilina Nikoulina. BERGEN: A benchmarking library for retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), _Findings of the Association for Computational Linguistics: EMNLP 2024_, pp. 7640‚Äì7663, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. [10.18653/v1/2024.findings-emnlp.449](https://arxiv.org/doi.org/10.18653/v1/2024.findings-emnlp.449). URL [https://aclanthology.org/2024.findings-emnlp.449/](https://aclanthology.org/2024.findings-emnlp.449/). 
*   Rau et al. (2024b) David Rau, Shuai Wang, Herv√© D√©jean, and St√©phane Clinchant. Context embeddings for efficient answer generation in rag, 2024b. URL [https://arxiv.org/abs/2407.09252](https://arxiv.org/abs/2407.09252). 
*   Seo et al. (2019) Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. Real-time open-domain question answering with dense-sparse phrase index. In Anna Korhonen, David Traum, and Llu√≠s M√†rquez (eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4430‚Äì4441, Florence, Italy, July 2019. Association for Computational Linguistics. [10.18653/v1/P19-1436](https://arxiv.org/doi.org/10.18653/v1/P19-1436). URL [https://aclanthology.org/P19-1436](https://aclanthology.org/P19-1436). 
*   Shitao et al. (2022) Xiao Shitao, Liu Zheng, Shao Yingxia, and Cao Zhao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. In _EMNLP_, 2022. URL [https://arxiv.org/abs/2205.12035](https://arxiv.org/abs/2205.12035). 
*   Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas R√ºckl√©, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. URL [https://openreview.net/forum?id=wCu6T5xFjeJ](https://openreview.net/forum?id=wCu6T5xFjeJ). 
*   Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 
*   Wang et al. (2023) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter context for retrieval-augmented generation, 2023. URL [https://arxiv.org/abs/2311.08377](https://arxiv.org/abs/2311.08377). 
*   Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 38‚Äì45, Online, October 2020. Association for Computational Linguistics. [10.18653/v1/2020.emnlp-demos.6](https://arxiv.org/doi.org/10.18653/v1/2020.emnlp-demos.6). URL [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6). 
*   Xu et al. (2024) Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=mlJLVigNHp](https://openreview.net/forum?id=mlJLVigNHp). 
*   Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 2369‚Äì2380, Brussels, Belgium, 2018. Association for Computational Linguistics. [10.18653/v1/D18-1259](https://arxiv.org/doi.org/10.18653/v1/D18-1259). URL [https://aclanthology.org/D18-1259](https://aclanthology.org/D18-1259). 
*   Yoon et al. (2024) Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering, 2024. URL [https://arxiv.org/abs/2407.09014](https://arxiv.org/abs/2407.09014). 
*   Yoran et al. (2024) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=ZS4m74kZpH](https://openreview.net/forum?id=ZS4m74kZpH). 
*   Zhu et al. (2024) Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. Accelerating inference of retrieval-augmented generation via sparse context selection. _ArXiv_, abs/2405.16178, 2024. URL [https://api.semanticscholar.org/CorpusID:270062557](https://api.semanticscholar.org/CorpusID:270062557). 

Appendix A Data
---------------

#### Evaluation datasets.

We consider the following datasets:

*   ‚Ä¢

Datasets with Wikipedia as a datastore:

    *   ‚Äì
    *   ‚Äì
    *   ‚ÄìPopQA(Mallen et al., [2023b](https://arxiv.org/html/2501.16214v1#bib.bib36)). We use a test set of 14 k ùëò k italic_k questions distributed by the dataset authors. 

*   ‚Ä¢

Datasets with individual datastores:

    *   ‚ÄìBioASQ(Nentidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib39)). We use a version of the dataset provided by(Hsia et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib19)), with 3.8 k ùëò k italic_k queries. We only use queries from categories ‚Äúyes/no‚Äù, ‚Äúfactoid‚Äù, and ‚Äúlist‚Äù. 
    *   ‚ÄìSyllabus QA(Fernandez et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib14)). We use the test set of 1.1 k ùëò k italic_k questions distributed by the authors; 
    *   ‚ÄìRGB(Chen et al., [2024b](https://arxiv.org/html/2501.16214v1#bib.bib5)). We use the test set of 200 English questions distributed by the authors. 

All datasets provide short answers (keywords) for each query, which we use to evaluate both match-based metrics such as Recall and LLM-based metrics Rau et al. ([2024a](https://arxiv.org/html/2501.16214v1#bib.bib44))9 9 9 Using SOLAR-10.7B(Kim et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib24))..

#### Datastores.

For training `Provence`, we use the MS MARCO document collection(Craswell et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib11)). We split each document into overlapping chunks of N ùëÅ N italic_N sentences, where N ùëÅ N italic_N is random in ‚àà1..10 absent 1..10\in 1..10‚àà 1..10 ‚Äì with a higher probability for longer contexts ‚Äì to train `Provence` on various context lengths. Each chunk is prepended with a page title. The resulting datastore contains 34 M ùëÄ M italic_M passages. We also process the Wikipedia datastore in a similar fashion, for ablation experiments. We download a 2024 Wikipedia dump and process it using scripts provided by Pyserini(Lin et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib32))10 10 10 At [https://github.com/castorini/pyserini/blob/master/docs/experiments-wiki-corpora.md](https://github.com/castorini/pyserini/blob/master/docs/experiments-wiki-corpora.md).. We also prepare versions of this Wikipedia datastore with passages of N ùëÅ N italic_N sentences with overlaps of N/2 ùëÅ 2 N/2 italic_N / 2 sentences, for testing `Provence` robustness to various context lengths.

All other evaluations on Wikipedia-based datasets ‚Äì including main evaluations ‚Äì are conducted on the Wikipedia datastore provided at [https://huggingface.co/datasets/castorini/odqa-wiki-corpora](https://huggingface.co/datasets/castorini/odqa-wiki-corpora). We use a version with passages of 6 sentences with a 3-sentence overlap ‚Äì making 9 M ùëÄ M italic_M passages in total.

For Pubmed, we use the version of the dataset provided by Hsia et al. ([2024](https://arxiv.org/html/2501.16214v1#bib.bib19)) at [https://huggingface.co/datasets/jenhsia/ragged](https://huggingface.co/datasets/jenhsia/ragged). It consists of 58 M ùëÄ M italic_M passages, extracted from Pubmed abstracts. Each passage (chunk) is prepended with the page‚Äôs title.

For SyllabusQA, we split each syllabus (provided by the authors) into passages of 100 words (each passage is prepended with the title). For RGB, the retrieved contexts are provided by the authors; we provide 3 relevant and 2 irrelevant contexts for each question (60% noise context).

Appendix B Models
-----------------

We list in Table[5](https://arxiv.org/html/2501.16214v1#A2.T5 "Table 5 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") all the main models used to conduct experiments for `Provence`.

Table 5: List of all the models used in the experiments with their corresponding HuggingFace checkpoints.

Table 6: Prompt used for generating silver labeling with LLaMA-3-8B-Instruct. The sentence citations in the response are parsed using regular expression.

Question: {question}
Context: [1] {sentence1} [2] {sentence2} [3] {sentence3} ‚Ä¶
Answer the Question, using ONLY information provided in the Context. If no useful information is provided, you MUST output "No answer". If some parts of the Context are used to answer, you MUST cite ALL the corresponding sentences. Use the symbols [ ] to indicate when a fact comes from a sentence in the context, e.g [0] for a fact from sentence 0. You should only answer the given question and should not provide any additional information.

![Image 8: Refer to caption](https://arxiv.org/html/2501.16214v1/x7.png)

![Image 9: Refer to caption](https://arxiv.org/html/2501.16214v1/x8.png)

Figure 5: Statistics of the silver contexts labeled by LLaMA-3-8B-Instruct. (Left) the distribution of the number of sentences in silver contexts. (Right) the distribution of the position of the selected sentences in contexts.

![Image 10: Refer to caption](https://arxiv.org/html/2501.16214v1/x9.png)

Figure 6: Example visualization of per-token probabilities of being selected in the final context.

![Image 11: Refer to caption](https://arxiv.org/html/2501.16214v1/x10.png)

Figure 7: Main results for various QA domains, comparing Provence and baseline models, metric: Recall. Generator: LLama-2-7B, retriever: SPLADE-v3, reranker: DeBERTa-v3 (or Provence in the unified setting). Plot titles denote ‚ÄúDataset name (datastore type)‚Äù. x ùë• x italic_x-axis denotes QA performance evaluated with Recall; y ùë¶ y italic_y-axis denotes the context compression ratio. For both metrics, the higher the better: the best model would be closest to the top right corner.

![Image 12: Refer to caption](https://arxiv.org/html/2501.16214v1/x11.png)

Figure 8: Testing Provence in various RAG settings (retrieval, re-ranking, generator).

![Image 13: Refer to caption](https://arxiv.org/html/2501.16214v1/x12.png)

Figure 9: Comparing Provence to a subset of baselines with retriever: RetroMAE(Shitao et al., [2022](https://arxiv.org/html/2501.16214v1#bib.bib47)), reranker: BGE-M3(Chen et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib4)), generator:: LLama-2-7B-chat.

![Image 14: Refer to caption](https://arxiv.org/html/2501.16214v1/x13.png)

Figure 10: Testing Provence with different top-k ùëò k italic_k documents provided to the generator. The setting is the same as the one in Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Table 7: nDCG@10 on the 13 open BEIR datasets.

Table 8: Numerical scores corresponding to Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") ‚Äì NQ, Hotpot QA, Tydi QA, and Pop QA.

NQ HotPot QA Tydi QA PopQA
LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %
Full context 71.8 0.0 57.0 0.0 73.9 0.0 57.8 0.0
Provence 72.4 62.2 56.7 66.4 70.5 63.0 59.3 68.6
(w/ reranker)72.6 76.0 56.0 82.4 73.6 76.2 59.5 75.8
Provence 72.3 64.1 56.6 69.5 70.9 65.8 59.0 69.9
(standalone)70.6 77.3 54.8 84.1 70.2 78.1 58.8 76.1
LLMLingua2 59.5 74.0 47.1 74.4 57.7 73.9 42.9 75.0
67.5 45.4 52.9 45.8 67.3 45.0 52.5 46.3
70.3 25.0 55.0 24.9 70.0 24.8 55.2 25.1
LongLLMLingua 61.3 69.1 52.6 68.5 56.6 69.5 49.5 65.5
68.5 47.9 55.6 46.5 65.5 47.8 54.5 43.6
71.3 28.7 56.9 26.8 69.1 28.8 57.6 23.9
RECOMP 70.6 43.6 55.5 40.9 68.6 46.4 56.9 41.1
(ext)68.2 59.8 53.4 57.5 67.0 63.0 55.7 57.4
66.2 77.1 50.1 75.7 64.5 79.7 52.3 74.9
RECOMP 69.0 59.5 50.9 62.9 70.9 52.4 54.8 56.0
(ext+thr)72.9 11.8 56.4 14.4 72.3 6.9 58.5 12.4
RECOMP (abs)66.9 94.5 53.1 94.4 66.4 95.2 54.4 92.8
DSLR 71.7 44.9 52.9 75.7 72.7 45.8 58.6 48.1
70.5 54.9 50.7 83.4 69.8 55.6 58.7 58.1
70.4 61.4 49.3 87.0 70.7 62.0 58.8 63.7
67.7 72.0 45.2 91.7 67.5 72.9 58.5 71.9
67.6 77.7 43.2 93.4 67.5 78.1 57.9 76.0
Dense-X retrieval 62.7 69.0 49.6 67.7 66.4 71.5 52.0 68.5

Table 9: Numerical scores corresponding to Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"): Syllabus QA, BioASQ, and RGB. 

Syllabus QA BioASQ RGB
LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %
Full context 52.9 0.0 80.7 0.0 93.5 0.0
Provence 49.8 60.6 80.6 49.0 94.4 60.5
(w/ reranker)51.0 76.5 80.3 67.4 96.3 69.3
Provence 50.7 64.1 80.6 51.3 95.8 61.6
(standalone)47.8 76.6 80.1 68.9 96.3 69.4
LLMLingua2 37.4 73.4 72.6 73.6 78.6 74.3
43.4 45.4 77.7 45.2 93.5 46.1
49.8 26.6 78.7 24.8 95.8 26.3
LongLLMLingua 42.3 71.3 72.2 72.9 71.6 73.9
45.1 48.5 77.3 50.4 83.3 51.6
50.9 29.2 78.7 31.3 92.1 32.1
RECOMP 44.6 51.5 78.7 42.2 97.7 42.0
(ext)42.7 61.4 78.4 54.8 94.9 52.1
39.1 71.1 76.3 69.7 94.4 63.2
RECOMP 45.5 35.7 76.6 51.2 92.1 51.4
(ext+thr)52.8 7.7 80.2 6.5 97.7 13.4
RECOMP (abs)38.1 98.9 68.2 96.1 90.7 95.7
DSLR 49.6 33.2 80.1 29.9 97.2 41.6
49.1 46.4 79.6 40.1 97.7 46.9
47.2 55.4 79.2 47.3 96.3 49.7
44.2 70.6 77.6 60.2 97.2 54.1
40.7 78.2 75.4 68.0 95.8 56.9

Table 10: Numerical scores corresponding to Figure[4](https://arxiv.org/html/2501.16214v1#S4.F4 "Figure 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). 

![Image 15: Refer to caption](https://arxiv.org/html/2501.16214v1/x14.png)

Figure 11: Ablation results with Recall (match-based metric).

Table 11: Example of context pruning with various approaches. Provence selects one sentence about the Shepard‚Äôs pie and removes sentences about other similar dishes, which is RECOMP (ext) is not capable of by design. RECOMP (abs) correctly generates a summary; LongLLMLingua removes the part relevant to the Shepard‚Äôs pie, and LLMLingua2 uniformly removes no-informative tokens.

Table 12: Example of context pruning with various approaches. Provence correctly detects that the entire passage is relevant to the query, same as LongLLMLingua, while RECOMP (ext) is by design not capable of making such a decision.

Table 13: Example of context pruning with various approaches. Provence selects one most relevant sentence, which is also ranked first by RECOMP (ext). RECOMP (abs) decides that no information is relevant to the query, while LongLLMLingua on the contrary keeps the entire input, dropping some punctuation marks. LLMLingua2 removes too many tokens which makes text hardly understandable.
