#### Limitations.

Despite `Provence` being ready to use in various settings, demonstrated in the paper, it is focusing only on QA applications, with a single passage processed at a time, and is trained on English-only data. Future work could consider extending it to other tasks, multi-passage contexts, and languages beyond English.

References
----------

*   AI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md). 
*   Anagnostidis et al. (2023) Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur√©lien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. _ArXiv_, abs/2305.15805, 2023. URL [https://api.semanticscholar.org/CorpusID:258888224](https://api.semanticscholar.org/CorpusID:258888224). 
*   Asai et al. (2024) Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. Reliable, adaptable, and attributable language models with retrieval. _arXiv preprint arXiv:2403.03187_, 2024. 
*   Chen et al. (2024a) Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024a. URL [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216). 
*   Chen et al. (2024b) Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. Benchmarking large language models in retrieval-augmented generation. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(16):17754‚Äì17762, Mar. 2024b. [10.1609/aaai.v38i16.29728](https://arxiv.org/doi.org/10.1609/aaai.v38i16.29728). URL [https://ojs.aaai.org/index.php/AAAI/article/view/29728](https://ojs.aaai.org/index.php/AAAI/article/view/29728). 
*   Chen et al. (2024c) Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. Dense X retrieval: What retrieval granularity should we use? In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), _Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing_, pp. 15159‚Äì15177, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. URL [https://aclanthology.org/2024.emnlp-main.845](https://aclanthology.org/2024.emnlp-main.845). 
*   Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 3829‚Äì3846, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.232](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.232). URL [https://aclanthology.org/2023.emnlp-main.232](https://aclanthology.org/2023.emnlp-main.232). 
*   Choi et al. (2021) Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. Decontextualization: Making sentences stand-alone. _Transactions of the Association for Computational Linguistics_, 9:447‚Äì461, 2021. [10.1162/tacl_a_00377](https://arxiv.org/doi.org/10.1162/tacl_a_00377). URL [https://aclanthology.org/2021.tacl-1.27](https://aclanthology.org/2021.tacl-1.27). 
*   Clark et al. (2020) Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. _Transactions of the Association for Computational Linguistics_, 8:454‚Äì470, 2020. [10.1162/tacl_a_00317](https://arxiv.org/doi.org/10.1162/tacl_a_00317). URL [https://aclanthology.org/2020.tacl-1.30](https://aclanthology.org/2020.tacl-1.30). 
*   Craswell et al. (2020) Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the trec 2019 deep learning track, 2020. URL [https://arxiv.org/abs/2003.07820](https://arxiv.org/abs/2003.07820). 
*   Craswell et al. (2021) Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. Ms marco: Benchmarking ranking models in the large-data regime. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR ‚Äô21, pp. 1566‚Äì1576, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380379. [10.1145/3404835.3462804](https://arxiv.org/doi.org/10.1145/3404835.3462804). URL [https://doi.org/10.1145/3404835.3462804](https://doi.org/10.1145/3404835.3462804). 
*   Dao (2024) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=mZn2Xyh9Ec](https://openreview.net/forum?id=mZn2Xyh9Ec). 
*   Das et al. (2019) Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. Multi-step retriever-reader interaction for scalable open-domain question answering. In _International Conference on Learning Representations_, 2019. URL [https://openreview.net/forum?id=HkfPSh05K7](https://openreview.net/forum?id=HkfPSh05K7). 
*   Fernandez et al. (2024) Nigel Fernandez, Alexander Scarlatos, and Andrew Lan. SyllabusQA: A course logistics question answering dataset. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 10344‚Äì10369, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [10.18653/v1/2024.acl-long.557](https://arxiv.org/doi.org/10.18653/v1/2024.acl-long.557). URL [https://aclanthology.org/2024.acl-long.557](https://aclanthology.org/2024.acl-long.557). 
*   Ge et al. (2024) Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=uREj4ZuGJE](https://openreview.net/forum?id=uREj4ZuGJE). 
*   He et al. (2021a) Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing, 2021a. 
*   He et al. (2021b) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentangled} {attention}. In _International Conference on Learning Representations_, 2021b. URL [https://openreview.net/forum?id=XPZIaotutsD](https://openreview.net/forum?id=XPZIaotutsD). 
*   Hofst√§tter et al. (2021) Sebastian Hofst√§tter, Sophia Althammer, Michael Schr√∂der, Mete Sertkan, and Allan Hanbury. Improving efficient neural ranking models with cross-architecture knowledge distillation, 2021. URL [https://arxiv.org/abs/2010.02666](https://arxiv.org/abs/2010.02666). 
*   Hsia et al. (2024) Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Graham Neubig. Ragged: Towards informed design of retrieval augmented generation systems. _arXiv preprint arXiv:2403.09040_, 2024. 
*   Hwang et al. (2024) Taeho Hwang, Soyeong Jeong, Sukmin Cho, SeungYoon Han, and Jong Park. DSLR: Document refinement with sentence-level re-ranking and reconstruction to enhance retrieval-augmented generation. In Wenhao Yu, Weijia Shi, Michihiro Yasunaga, Meng Jiang, Chenguang Zhu, Hannaneh Hajishirzi, Luke Zettlemoyer, and Zhihan Zhang (eds.), _Proceedings of the 3rd Workshop on Knowledge Augmented Methods for NLP_, pp. 73‚Äì92, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [10.18653/v1/2024.knowledgenlp-1.6](https://arxiv.org/doi.org/10.18653/v1/2024.knowledgenlp-1.6). URL [https://aclanthology.org/2024.knowledgenlp-1.6](https://aclanthology.org/2024.knowledgenlp-1.6). 
*   Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot Learning with Retrieval Augmented Language Models, November 2022. URL [http://arxiv.org/abs/2208.03299](http://arxiv.org/abs/2208.03299). arXiv:2208.03299 [cs]. 
*   Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LLMLingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 13358‚Äì13376, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.825](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.825). URL [https://aclanthology.org/2023.emnlp-main.825](https://aclanthology.org/2023.emnlp-main.825). 
*   Jiang et al. (2024) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1658‚Äì1677, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL [https://aclanthology.org/2024.acl-long.91](https://aclanthology.org/2024.acl-long.91). 
*   Kim et al. (2023) Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling, 2023. 
*   Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453‚Äì466, 2019. 
*   Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. 
*   (27) LangChain. LangChain Documentation. [https://python.langchain.com/](https://python.langchain.com/). 
*   Lassance & Clinchant (2023) Carlos Lassance and St√©phane Clinchant. Naver labs europe (splade) @ trec deep learning 2022, 2023. URL [https://arxiv.org/abs/2302.12574](https://arxiv.org/abs/2302.12574). 
*   Lassance et al. (2024) Carlos Lassance, Herv√© D√©jean, Thibault Formal, and St√©phane Clinchant. Splade-v3: New baselines for splade, 2024. 
*   Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In _Advances in Neural Information Processing Systems_, volume 33, pp. 9459‚Äì9474. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html). 
*   Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 6342‚Äì6353, Singapore, December 2023. Association for Computational Linguistics. [10.18653/v1/2023.emnlp-main.391](https://arxiv.org/doi.org/10.18653/v1/2023.emnlp-main.391). URL [https://aclanthology.org/2023.emnlp-main.391](https://aclanthology.org/2023.emnlp-main.391). 
*   Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. Pyserini: A python toolkit for reproducible information retrieval research with sparse and dense representations. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR ‚Äô21, pp. 2356‚Äì2362, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380379. [10.1145/3404835.3463238](https://arxiv.org/doi.org/10.1145/3404835.3463238). URL [https://doi.org/10.1145/3404835.3463238](https://doi.org/10.1145/3404835.3463238). 
*   Lin et al. (2024) Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-augmented dual instruction tuning. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=22OTbutug9](https://openreview.net/forum?id=22OTbutug9). 
*   Louis et al. (2025) Maxime Louis, Herv√© D√©jean, and St√©phane Clinchant. Pisco: Pretty simple compression for retrieval-augmented generation. _arXiv preprint_, 2025. 
*   Mallen et al. (2023a) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9802‚Äì9822, Toronto, Canada, July 2023a. Association for Computational Linguistics. [10.18653/v1/2023.acl-long.546](https://arxiv.org/doi.org/10.18653/v1/2023.acl-long.546). URL [https://aclanthology.org/2023.acl-long.546](https://aclanthology.org/2023.acl-long.546). 
*   Mallen et al. (2023b) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9802‚Äì9822, Toronto, Canada, July 2023b. Association for Computational Linguistics. [10.18653/v1/2023.acl-long.546](https://arxiv.org/doi.org/10.18653/v1/2023.acl-long.546). URL [https://aclanthology.org/2023.acl-long.546](https://aclanthology.org/2023.acl-long.546). 
*   Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation, 2023. URL [https://arxiv.org/abs/2305.14251](https://arxiv.org/abs/2305.14251). 
*   Nawrot et al. (2024) Piotr Nawrot, Adrian La‚Äôncucki, Marcin Chochowski, David Tarjan, and E.Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference. _ArXiv_, abs/2403.09636, 2024. URL [https://api.semanticscholar.org/CorpusID:268384862](https://api.semanticscholar.org/CorpusID:268384862). 
*   Nentidis et al. (2023) Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima-L√≥pez, Eulalia Farr√©-Maduell, Luis Gasco Sanchez, Martin Krallinger, and Georgios Paliouras. _Overview of BioASQ 2023: The Eleventh BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering_, pp. 227‚Äì250. 09 2023. ISBN 978-3-031-42447-2. [10.1007/978-3-031-42448-9_19](https://arxiv.org/doi.org/10.1007/978-3-031-42448-9_19). 
*   Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human generated machine reading comprehension dataset. November 2016. URL [https://microsoft.github.io/msmarco/](https://microsoft.github.io/msmarco/). 
*   Nogueira & Cho (2020) Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert, 2020. 
*   Pan et al. (2024) Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H.Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Findings of the Association for Computational Linguistics ACL 2024_, pp. 963‚Äì981, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-acl.57](https://aclanthology.org/2024.findings-acl.57). 
*   Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K√∂pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. _PyTorch: an imperative style, high-performance deep learning library_. Curran Associates Inc., Red Hook, NY, USA, 2019. 
*   Rau et al. (2024a) David Rau, Herv√© D√©jean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, St√©phane Clinchant, and Vassilina Nikoulina. BERGEN: A benchmarking library for retrieval-augmented generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), _Findings of the Association for Computational Linguistics: EMNLP 2024_, pp. 7640‚Äì7663, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. [10.18653/v1/2024.findings-emnlp.449](https://arxiv.org/doi.org/10.18653/v1/2024.findings-emnlp.449). URL [https://aclanthology.org/2024.findings-emnlp.449/](https://aclanthology.org/2024.findings-emnlp.449/). 
*   Rau et al. (2024b) David Rau, Shuai Wang, Herv√© D√©jean, and St√©phane Clinchant. Context embeddings for efficient answer generation in rag, 2024b. URL [https://arxiv.org/abs/2407.09252](https://arxiv.org/abs/2407.09252). 
*   Seo et al. (2019) Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. Real-time open-domain question answering with dense-sparse phrase index. In Anna Korhonen, David Traum, and Llu√≠s M√†rquez (eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4430‚Äì4441, Florence, Italy, July 2019. Association for Computational Linguistics. [10.18653/v1/P19-1436](https://arxiv.org/doi.org/10.18653/v1/P19-1436). URL [https://aclanthology.org/P19-1436](https://aclanthology.org/P19-1436). 
*   Shitao et al. (2022) Xiao Shitao, Liu Zheng, Shao Yingxia, and Cao Zhao. Retromae: Pre-training retrieval-oriented language models via masked auto-encoder. In _EMNLP_, 2022. URL [https://arxiv.org/abs/2205.12035](https://arxiv.org/abs/2205.12035). 
*   Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas R√ºckl√©, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. URL [https://openreview.net/forum?id=wCu6T5xFjeJ](https://openreview.net/forum?id=wCu6T5xFjeJ). 
*   Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. 
*   Wang et al. (2023) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. Learning to filter context for retrieval-augmented generation, 2023. URL [https://arxiv.org/abs/2311.08377](https://arxiv.org/abs/2311.08377). 
*   Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 38‚Äì45, Online, October 2020. Association for Computational Linguistics. [10.18653/v1/2020.emnlp-demos.6](https://arxiv.org/doi.org/10.18653/v1/2020.emnlp-demos.6). URL [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6). 
*   Xu et al. (2024) Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=mlJLVigNHp](https://openreview.net/forum?id=mlJLVigNHp). 
*   Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun‚Äôichi Tsujii (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp. 2369‚Äì2380, Brussels, Belgium, 2018. Association for Computational Linguistics. [10.18653/v1/D18-1259](https://arxiv.org/doi.org/10.18653/v1/D18-1259). URL [https://aclanthology.org/D18-1259](https://aclanthology.org/D18-1259). 
*   Yoon et al. (2024) Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering, 2024. URL [https://arxiv.org/abs/2407.09014](https://arxiv.org/abs/2407.09014). 
*   Yoran et al. (2024) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust to irrelevant context. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=ZS4m74kZpH](https://openreview.net/forum?id=ZS4m74kZpH). 
*   Zhu et al. (2024) Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. Accelerating inference of retrieval-augmented generation via sparse context selection. _ArXiv_, abs/2405.16178, 2024. URL [https://api.semanticscholar.org/CorpusID:270062557](https://api.semanticscholar.org/CorpusID:270062557). 

Appendix A Data
---------------

#### Evaluation datasets.

We consider the following datasets:

*   ‚Ä¢

Datasets with Wikipedia as a datastore:

    *   ‚Äì
    *   ‚Äì
    *   ‚ÄìPopQA(Mallen et al., [2023b](https://arxiv.org/html/2501.16214v1#bib.bib36)). We use a test set of 14 k ùëò k italic_k questions distributed by the dataset authors. 

*   ‚Ä¢

Datasets with individual datastores:

    *   ‚ÄìBioASQ(Nentidis et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib39)). We use a version of the dataset provided by(Hsia et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib19)), with 3.8 k ùëò k italic_k queries. We only use queries from categories ‚Äúyes/no‚Äù, ‚Äúfactoid‚Äù, and ‚Äúlist‚Äù. 
    *   ‚ÄìSyllabus QA(Fernandez et al., [2024](https://arxiv.org/html/2501.16214v1#bib.bib14)). We use the test set of 1.1 k ùëò k italic_k questions distributed by the authors; 
    *   ‚ÄìRGB(Chen et al., [2024b](https://arxiv.org/html/2501.16214v1#bib.bib5)). We use the test set of 200 English questions distributed by the authors. 

All datasets provide short answers (keywords) for each query, which we use to evaluate both match-based metrics such as Recall and LLM-based metrics Rau et al. ([2024a](https://arxiv.org/html/2501.16214v1#bib.bib44))9 9 9 Using SOLAR-10.7B(Kim et al., [2023](https://arxiv.org/html/2501.16214v1#bib.bib24))..

#### Datastores.

For training `Provence`, we use the MS MARCO document collection(Craswell et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib11)). We split each document into overlapping chunks of N ùëÅ N italic_N sentences, where N ùëÅ N italic_N is random in ‚àà1..10 absent 1..10\in 1..10‚àà 1..10 ‚Äì with a higher probability for longer contexts ‚Äì to train `Provence` on various context lengths. Each chunk is prepended with a page title. The resulting datastore contains 34 M ùëÄ M italic_M passages. We also process the Wikipedia datastore in a similar fashion, for ablation experiments. We download a 2024 Wikipedia dump and process it using scripts provided by Pyserini(Lin et al., [2021](https://arxiv.org/html/2501.16214v1#bib.bib32))10 10 10 At [https://github.com/castorini/pyserini/blob/master/docs/experiments-wiki-corpora.md](https://github.com/castorini/pyserini/blob/master/docs/experiments-wiki-corpora.md).. We also prepare versions of this Wikipedia datastore with passages of N ùëÅ N italic_N sentences with overlaps of N/2 ùëÅ 2 N/2 italic_N / 2 sentences, for testing `Provence` robustness to various context lengths.

All other evaluations on Wikipedia-based datasets ‚Äì including main evaluations ‚Äì are conducted on the Wikipedia datastore provided at [https://huggingface.co/datasets/castorini/odqa-wiki-corpora](https://huggingface.co/datasets/castorini/odqa-wiki-corpora). We use a version with passages of 6 sentences with a 3-sentence overlap ‚Äì making 9 M ùëÄ M italic_M passages in total.

For Pubmed, we use the version of the dataset provided by Hsia et al. ([2024](https://arxiv.org/html/2501.16214v1#bib.bib19)) at [https://huggingface.co/datasets/jenhsia/ragged](https://huggingface.co/datasets/jenhsia/ragged). It consists of 58 M ùëÄ M italic_M passages, extracted from Pubmed abstracts. Each passage (chunk) is prepended with the page‚Äôs title.

For SyllabusQA, we split each syllabus (provided by the authors) into passages of 100 words (each passage is prepended with the title). For RGB, the retrieved contexts are provided by the authors; we provide 3 relevant and 2 irrelevant contexts for each question (60% noise context).

Appendix B Models
-----------------

We list in Table[5](https://arxiv.org/html/2501.16214v1#A2.T5 "Table 5 ‚Ä£ Appendix B Models ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") all the main models used to conduct experiments for `Provence`.

Table 5: List of all the models used in the experiments with their corresponding HuggingFace checkpoints.

Table 6: Prompt used for generating silver labeling with LLaMA-3-8B-Instruct. The sentence citations in the response are parsed using regular expression.

Question: {question}
Context: [1] {sentence1} [2] {sentence2} [3] {sentence3} ‚Ä¶
Answer the Question, using ONLY information provided in the Context. If no useful information is provided, you MUST output "No answer". If some parts of the Context are used to answer, you MUST cite ALL the corresponding sentences. Use the symbols [ ] to indicate when a fact comes from a sentence in the context, e.g [0] for a fact from sentence 0. You should only answer the given question and should not provide any additional information.

![Image 8: Refer to caption](https://arxiv.org/html/2501.16214v1/x7.png)

![Image 9: Refer to caption](https://arxiv.org/html/2501.16214v1/x8.png)

Figure 5: Statistics of the silver contexts labeled by LLaMA-3-8B-Instruct. (Left) the distribution of the number of sentences in silver contexts. (Right) the distribution of the position of the selected sentences in contexts.

![Image 10: Refer to caption](https://arxiv.org/html/2501.16214v1/x9.png)

Figure 6: Example visualization of per-token probabilities of being selected in the final context.

![Image 11: Refer to caption](https://arxiv.org/html/2501.16214v1/x10.png)

Figure 7: Main results for various QA domains, comparing Provence and baseline models, metric: Recall. Generator: LLama-2-7B, retriever: SPLADE-v3, reranker: DeBERTa-v3 (or Provence in the unified setting). Plot titles denote ‚ÄúDataset name (datastore type)‚Äù. x ùë• x italic_x-axis denotes QA performance evaluated with Recall; y ùë¶ y italic_y-axis denotes the context compression ratio. For both metrics, the higher the better: the best model would be closest to the top right corner.

![Image 12: Refer to caption](https://arxiv.org/html/2501.16214v1/x11.png)

Figure 8: Testing Provence in various RAG settings (retrieval, re-ranking, generator).

![Image 13: Refer to caption](https://arxiv.org/html/2501.16214v1/x12.png)

Figure 9: Comparing Provence to a subset of baselines with retriever: RetroMAE(Shitao et al., [2022](https://arxiv.org/html/2501.16214v1#bib.bib47)), reranker: BGE-M3(Chen et al., [2024a](https://arxiv.org/html/2501.16214v1#bib.bib4)), generator:: LLama-2-7B-chat.

![Image 14: Refer to caption](https://arxiv.org/html/2501.16214v1/x13.png)

Figure 10: Testing Provence with different top-k ùëò k italic_k documents provided to the generator. The setting is the same as the one in Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation").

Table 7: nDCG@10 on the 13 open BEIR datasets.

Table 8: Numerical scores corresponding to Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation") ‚Äì NQ, Hotpot QA, Tydi QA, and Pop QA.

NQ HotPot QA Tydi QA PopQA
LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %
Full context 71.8 0.0 57.0 0.0 73.9 0.0 57.8 0.0
Provence 72.4 62.2 56.7 66.4 70.5 63.0 59.3 68.6
(w/ reranker)72.6 76.0 56.0 82.4 73.6 76.2 59.5 75.8
Provence 72.3 64.1 56.6 69.5 70.9 65.8 59.0 69.9
(standalone)70.6 77.3 54.8 84.1 70.2 78.1 58.8 76.1
LLMLingua2 59.5 74.0 47.1 74.4 57.7 73.9 42.9 75.0
67.5 45.4 52.9 45.8 67.3 45.0 52.5 46.3
70.3 25.0 55.0 24.9 70.0 24.8 55.2 25.1
LongLLMLingua 61.3 69.1 52.6 68.5 56.6 69.5 49.5 65.5
68.5 47.9 55.6 46.5 65.5 47.8 54.5 43.6
71.3 28.7 56.9 26.8 69.1 28.8 57.6 23.9
RECOMP 70.6 43.6 55.5 40.9 68.6 46.4 56.9 41.1
(ext)68.2 59.8 53.4 57.5 67.0 63.0 55.7 57.4
66.2 77.1 50.1 75.7 64.5 79.7 52.3 74.9
RECOMP 69.0 59.5 50.9 62.9 70.9 52.4 54.8 56.0
(ext+thr)72.9 11.8 56.4 14.4 72.3 6.9 58.5 12.4
RECOMP (abs)66.9 94.5 53.1 94.4 66.4 95.2 54.4 92.8
DSLR 71.7 44.9 52.9 75.7 72.7 45.8 58.6 48.1
70.5 54.9 50.7 83.4 69.8 55.6 58.7 58.1
70.4 61.4 49.3 87.0 70.7 62.0 58.8 63.7
67.7 72.0 45.2 91.7 67.5 72.9 58.5 71.9
67.6 77.7 43.2 93.4 67.5 78.1 57.9 76.0
Dense-X retrieval 62.7 69.0 49.6 67.7 66.4 71.5 52.0 68.5

Table 9: Numerical scores corresponding to Figure[2](https://arxiv.org/html/2501.16214v1#S3.F2 "Figure 2 ‚Ä£ 3 Provence ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"): Syllabus QA, BioASQ, and RGB. 

Syllabus QA BioASQ RGB
LLM-Eval Comp. rate %LLM-Eval Comp. rate %LLM-Eval Comp. rate %
Full context 52.9 0.0 80.7 0.0 93.5 0.0
Provence 49.8 60.6 80.6 49.0 94.4 60.5
(w/ reranker)51.0 76.5 80.3 67.4 96.3 69.3
Provence 50.7 64.1 80.6 51.3 95.8 61.6
(standalone)47.8 76.6 80.1 68.9 96.3 69.4
LLMLingua2 37.4 73.4 72.6 73.6 78.6 74.3
43.4 45.4 77.7 45.2 93.5 46.1
49.8 26.6 78.7 24.8 95.8 26.3
LongLLMLingua 42.3 71.3 72.2 72.9 71.6 73.9
45.1 48.5 77.3 50.4 83.3 51.6
50.9 29.2 78.7 31.3 92.1 32.1
RECOMP 44.6 51.5 78.7 42.2 97.7 42.0
(ext)42.7 61.4 78.4 54.8 94.9 52.1
39.1 71.1 76.3 69.7 94.4 63.2
RECOMP 45.5 35.7 76.6 51.2 92.1 51.4
(ext+thr)52.8 7.7 80.2 6.5 97.7 13.4
RECOMP (abs)38.1 98.9 68.2 96.1 90.7 95.7
DSLR 49.6 33.2 80.1 29.9 97.2 41.6
49.1 46.4 79.6 40.1 97.7 46.9
47.2 55.4 79.2 47.3 96.3 49.7
44.2 70.6 77.6 60.2 97.2 54.1
40.7 78.2 75.4 68.0 95.8 56.9

Table 10: Numerical scores corresponding to Figure[4](https://arxiv.org/html/2501.16214v1#S4.F4 "Figure 4 ‚Ä£ 4.3 Analysis ‚Ä£ 4 Experiments ‚Ä£ Provence: efficient and robust context pruning for retrieval-augmented generation"). 

![Image 15: Refer to caption](https://arxiv.org/html/2501.16214v1/x14.png)

Figure 11: Ablation results with Recall (match-based metric).

Table 11: Example of context pruning with various approaches. Provence selects one sentence about the Shepard‚Äôs pie and removes sentences about other similar dishes, which is RECOMP (ext) is not capable of by design. RECOMP (abs) correctly generates a summary; LongLLMLingua removes the part relevant to the Shepard‚Äôs pie, and LLMLingua2 uniformly removes no-informative tokens.

Table 12: Example of context pruning with various approaches. Provence correctly detects that the entire passage is relevant to the query, same as LongLLMLingua, while RECOMP (ext) is by design not capable of making such a decision.

Table 13: Example of context pruning with various approaches. Provence selects one most relevant sentence, which is also ranked first by RECOMP (ext). RECOMP (abs) decides that no information is relevant to the query, while LongLLMLingua on the contrary keeps the entire input, dropping some punctuation marks. LLMLingua2 removes too many tokens which makes text hardly understandable.
